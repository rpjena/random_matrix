{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSUM Regime Detection on S&P 500\n## GARCH-Filtered Residuals \u00b7 Portfolio-Level Signal \u00b7 Calibrated Cooldown \u00b7 Backtest\n\n**Methodology:**\n1. Simulate S&P 500-like returns with known regime structure (swap `yfinance` in for live data)\n2. Fit GARCH(1,1) and use standardised residuals to remove volatility clustering\n3. Run two-sided CUSUM with threshold *h* calibrated to a target ARL\n4. Calibrate cooldown period from Monte Carlo inter-signal gap distribution\n5. Backtest as regime-switching strategy and evaluate signal quality vs true regimes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports & Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.gridspec as gridspec\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\nnp.random.seed(42)\nplt.rcParams.update({\n    \"figure.facecolor\": \"#0f1117\", \"axes.facecolor\": \"#0f1117\",\n    \"axes.edgecolor\": \"#444\", \"axes.labelcolor\": \"#ccc\",\n    \"xtick.color\": \"#aaa\", \"ytick.color\": \"#aaa\", \"text.color\": \"#eee\",\n    \"grid.color\": \"#2a2a2a\", \"grid.linestyle\": \"--\",\n    \"lines.linewidth\": 1.4, \"font.family\": \"monospace\",\n})\nACCENT = \"#00d4aa\"; RED = \"#ff4d6d\"; YELLOW = \"#ffd166\"\nBLUE = \"#4cc9f0\"; PURPLE = \"#9d4edd\"\nprint(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data \u2014 Simulated S&P 500 (2000-2024)\n\nReturns are simulated with **explicit regime structure** matching S&P 500 history.\nEach regime has its own (mu, sigma) and GARCH-like vol dynamics.\n\n| Regime | Char |\n|---|---|\n| Dot-com bust 2000-2002 | Negative drift, elevated vol |\n| Bull 2004-2007 | Low vol, steady gains |\n| GFC crash 2008-2009 | Severe crash, extreme vol |\n| QE recovery 2010-2019 | Long bull market |\n| COVID crash Q1 2020 | Fastest bear on record |\n| Rate hike bear 2022 | Negative, elevated vol |\n| AI bull 2023-2024 | Strong positive |\n\n> **Real data**: Uncomment the `yfinance` block and comment out `simulate_sp500()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Real data swap-in (requires: pip install yfinance arch) ----\n# import yfinance as yf\n# raw     = yf.download(\"^GSPC\", start=\"2000-01-01\", end=\"2024-12-31\", auto_adjust=True)\n# prices  = raw[\"Close\"].dropna()\n# returns = np.log(prices / prices.shift(1)).dropna()\n# true_regimes = None   # no ground-truth labels for real data\n# -----------------------------------------------------------------\n\ndef simulate_sp500(seed=42):\n    \"\"\"Simulate S&P 500-like daily log-returns with realistic regime structure.\"\"\"\n    rng = np.random.default_rng(seed)\n    # (label, trading_days, daily_mu, daily_sigma, GARCH_alpha, GARCH_beta)\n    regimes = [\n        (\"Dot-com bust\",   504, -0.0008, 0.016, 0.12, 0.82),\n        (\"Recovery 2003\",  252,  0.0005, 0.010, 0.07, 0.88),\n        (\"Bull 2004-07\",  1008,  0.0004, 0.008, 0.06, 0.90),\n        (\"GFC crash\",      504, -0.0015, 0.025, 0.18, 0.76),\n        (\"QE recovery\",   2520,  0.0005, 0.009, 0.07, 0.89),\n        (\"COVID crash\",     60, -0.0040, 0.040, 0.22, 0.70),\n        (\"COVID recovery\", 192,  0.0015, 0.015, 0.10, 0.84),\n        (\"Bull 2021\",      252,  0.0006, 0.010, 0.06, 0.90),\n        (\"Rate hike bear\", 252, -0.0008, 0.017, 0.13, 0.81),\n        (\"AI bull\",        504,  0.0007, 0.010, 0.07, 0.89),\n    ]\n    all_rets, all_labels, all_dates = [], [], []\n    date = pd.Timestamp(\"2000-01-03\")\n    for label, ndays, mu, sigma, alpha, beta in regimes:\n        h = sigma**2\n        omega = sigma**2 * (1 - alpha - beta)\n        eps = rng.standard_normal(ndays)\n        rets = np.zeros(ndays)\n        for t in range(ndays):\n            h = omega + alpha*(rets[t-1]**2 if t > 0 else 0) + beta*h\n            h = max(h, 1e-8)\n            rets[t] = mu + np.sqrt(h)*eps[t]\n        for r in rets:\n            while date.weekday() >= 5:\n                date += pd.Timedelta(days=1)\n            all_rets.append(r); all_labels.append(label); all_dates.append(date)\n            date += pd.Timedelta(days=1)\n    idx = pd.DatetimeIndex(all_dates)\n    return (pd.Series(all_rets, index=idx, name=\"SP500\"),\n            pd.Series(all_labels, index=idx, name=\"regime\"))\n\nreturns, true_regimes = simulate_sp500()\nprice_index = 1000 * np.exp(returns.cumsum())\n\nprint(f\"Date range    : {returns.index[0].date()} to {returns.index[-1].date()}\")\nprint(f\"Observations  : {len(returns):,}\")\nprint(f\"Ann. return   : {returns.mean()*252:.2%}\")\nprint(f\"Ann. vol      : {returns.std()*np.sqrt(252):.2%}\")\nprint(f\"Skewness      : {returns.skew():.3f}\")\nprint(f\"Excess kurt.  : {returns.kurt():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_palette = {\n    \"Dot-com bust\": \"#ff4d6d22\", \"Recovery 2003\": \"#00d4aa22\",\n    \"Bull 2004-07\": \"#00d4aa22\", \"GFC crash\": \"#ff4d6d22\",\n    \"QE recovery\":  \"#00d4aa22\", \"COVID crash\": \"#ff4d6d22\",\n    \"COVID recovery\": \"#ffd16622\", \"Bull 2021\": \"#00d4aa22\",\n    \"Rate hike bear\": \"#ff4d6d22\", \"AI bull\": \"#00d4aa22\",\n}\n\nfig, axes = plt.subplots(2, 1, figsize=(16, 8), sharex=True)\n\nax = axes[0]\nax.plot(price_index.index, price_index.values, color=ACCENT, lw=1.2)\nax.set_ylabel(\"Price Index (log)\"); ax.set_yscale(\"log\")\nax.set_title(\"Simulated S&P 500 \u2014 Price Index 2000-2024\", fontsize=13)\nprev_r = true_regimes.iloc[0]; seg_start = true_regimes.index[0]\nfor i, (dt, r) in enumerate(true_regimes.items()):\n    if r != prev_r or i == len(true_regimes)-1:\n        ax.axvspan(seg_start, dt, color=regime_palette.get(prev_r, \"#fff1\"), lw=0)\n        seg_start = dt; prev_r = r\nax.grid(True, alpha=0.4)\n\nax = axes[1]\nax.fill_between(returns.index, returns.values, 0,\n    where=returns.values >= 0, color=ACCENT, alpha=0.6, lw=0)\nax.fill_between(returns.index, returns.values, 0,\n    where=returns.values < 0, color=RED, alpha=0.6, lw=0)\nax.set_ylabel(\"Log Return\"); ax.set_title(\"Daily Log Returns\"); ax.grid(True, alpha=0.4)\nplt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GARCH(1,1) Filtering\n\nRaw returns exhibit **volatility clustering** \u2014 sustained high-vol periods create large\ndeviations that trigger CUSUM even without a mean regime change.\n\nWe fit GARCH(1,1) by maximum likelihood and use the **standardised residuals**:\n$$z_t = \\varepsilon_t / \\sigma_t, \\quad \\text{where } \\sigma_t^2 = \\omega + \\alpha\\varepsilon_{t-1}^2 + \\beta\\sigma_{t-1}^2$$\n\nAfter filtering, $z_t \\approx \\mathcal{N}(0,1)$ under stable conditions. Regime changes\nthen appear as persistent shifts in the *mean* of $z_t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def garch_loglik(params, r):\n    \"\"\"Negative log-likelihood for GARCH(1,1) with Gaussian innovations.\"\"\"\n    mu, omega, alpha, beta = params\n    if omega <= 0 or alpha < 0 or beta < 0 or alpha+beta >= 1:\n        return 1e10\n    n = len(r); e = r - mu\n    h = np.full(n, np.var(r))\n    ll = 0.0\n    for t in range(1, n):\n        h[t] = omega + alpha*e[t-1]**2 + beta*h[t-1]\n        if h[t] <= 0: return 1e10\n        ll += -0.5*(np.log(2*np.pi) + np.log(h[t]) + e[t]**2/h[t])\n    return -ll\n\ndef fit_garch(r):\n    \"\"\"Fit GARCH(1,1) via L-BFGS-B. Returns params, conditional vol, z-residuals.\"\"\"\n    rv = np.var(r)\n    x0 = [np.mean(r), rv*0.05, 0.08, 0.88]\n    bnds = [(None,None),(1e-7,None),(1e-5,0.5),(1e-5,0.998)]\n    res = minimize(garch_loglik, x0, args=(r,), method=\"L-BFGS-B\",\n                   bounds=bnds, options={\"maxiter\":5000,\"ftol\":1e-10})\n    mu, omega, alpha, beta = res.x\n    n = len(r); e = r - mu\n    h = np.full(n, np.var(r))\n    for t in range(1, n):\n        h[t] = max(omega + alpha*e[t-1]**2 + beta*h[t-1], 1e-8)\n    sigma = np.sqrt(h)\n    z = e / sigma\n    return {\"mu\":mu,\"omega\":omega,\"alpha\":alpha,\"beta\":beta}, sigma, z\n\nr_arr = returns.values\ngarch_params, cond_vol, z_resid = fit_garch(r_arr)\ncond_vol_s = pd.Series(cond_vol, index=returns.index)\nz_s        = pd.Series(z_resid,  index=returns.index)\n\ngp = garch_params\nprint(\"GARCH(1,1) fitted parameters:\")\nprint(f\"  mu    = {gp['mu']:.6f}  ({gp['mu']*252:.3%} annualised)\")\nprint(f\"  omega = {gp['omega']:.3e}\")\nprint(f\"  alpha = {gp['alpha']:.4f}  (ARCH effect)\")\nprint(f\"  beta  = {gp['beta']:.4f}  (GARCH effect)\")\nprint(f\"  persistence (alpha+beta) = {gp['alpha']+gp['beta']:.4f}  (<1 = covariance stationary)\")\nuncond = np.sqrt(gp[\"omega\"]/(1-gp[\"alpha\"]-gp[\"beta\"])) * np.sqrt(252)\nprint(f\"  Unconditional ann. vol   = {uncond:.2%}\")\nprint(f\"\\nResiduals z_t: mean={z_resid.mean():.4f}, std={z_resid.std():.4f}, \"\n      f\"kurt={pd.Series(z_resid).kurt():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n\n# Conditional vol\nax = axes[0,0]\nv = cond_vol_s * np.sqrt(252) * 100\nax.plot(v.index, v.values, color=YELLOW, lw=0.8)\nax.fill_between(v.index, v.values, alpha=0.3, color=YELLOW)\nax.set_title(\"Conditional Volatility (Ann. %)\"); ax.set_ylabel(\"Vol %\"); ax.grid(True, alpha=0.4)\n\n# Standardised residuals\nax = axes[0,1]\nax.plot(z_s.index, z_s.values, color=BLUE, lw=0.4, alpha=0.8)\nax.axhline(0, color=\"white\", lw=0.8, ls=\"--\")\nax.axhline( 3, color=RED, lw=0.8, ls=\":\"); ax.axhline(-3, color=RED, lw=0.8, ls=\":\")\nax.set_title(\"Standardised Residuals z_t\"); ax.set_ylabel(\"z\"); ax.grid(True, alpha=0.4)\n\n# QQ plot\nax = axes[1,0]\nqq = stats.probplot(z_resid)\nax.scatter(qq[0][0], qq[0][1], color=ACCENT, s=2, alpha=0.4)\nlim = max(abs(qq[0][0].min()), abs(qq[0][0].max()))\nax.plot([-lim,lim],[qq[1][0]*(-lim)+qq[1][1], qq[1][0]*lim+qq[1][1]], color=RED, lw=1.5)\nax.set_title(\"QQ Plot (vs Normal)\"); ax.set_xlabel(\"Theoretical Quantiles\"); ax.grid(True, alpha=0.4)\n\n# ACF of z^2 (ARCH effects)\nax = axes[1,1]\nz2 = z_resid**2; lags = range(1,31)\nacf = [pd.Series(z2).autocorr(lag=l) for l in lags]\nci  = 1.96/np.sqrt(len(z2))\nax.bar(lags, acf, color=PURPLE, alpha=0.7)\nax.axhline(ci, color=YELLOW, ls=\"--\", lw=1, label=\"95% CI\")\nax.axhline(-ci, color=YELLOW, ls=\"--\", lw=1)\nax.set_title(\"ACF of z^2  (residual ARCH effects?)\"); ax.set_xlabel(\"Lag\"); ax.legend()\nax.grid(True, alpha=0.4)\n\nplt.suptitle(\"GARCH(1,1) Diagnostics\", fontsize=14, y=1.01)\nplt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CUSUM Engine\n\nTwo-sided CUSUM on the GARCH standardised residuals:\n\n$$S^+_t = \\max(0,\\; S^+_{t-1} + z_t - k) \\qquad \\text{detects upward shift}$$\n$$S^-_t = \\max(0,\\; S^-_{t-1} - z_t - k) \\qquad \\text{detects downward shift}$$\n\n- Signal fires when $S^\\pm_t > h$\n- After a signal both statistics **reset to 0**\n- **Cooldown** prevents re-triggering within a specified window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cusum(z, h, k, cooldown=0):\n    \"\"\"\n    Two-sided CUSUM with cooldown.\n    Returns dict: S_pos, S_neg, signals (1=up, -1=down, 0=none)\n    \"\"\"\n    z = np.asarray(z); n = len(z)\n    S_pos = np.zeros(n); S_neg = np.zeros(n)\n    signals = np.zeros(n, dtype=int)\n    last_signal = -cooldown - 1\n    for t in range(1, n):\n        S_pos[t] = max(0, S_pos[t-1] + z[t] - k)\n        S_neg[t] = max(0, S_neg[t-1] - z[t] - k)\n        if (t - last_signal) > cooldown:\n            if S_pos[t] > h:\n                signals[t] = 1; last_signal = t\n                S_pos[t] = 0; S_neg[t] = 0\n            elif S_neg[t] > h:\n                signals[t] = -1; last_signal = t\n                S_pos[t] = 0; S_neg[t] = 0\n    return {\"S_pos\": S_pos, \"S_neg\": S_neg, \"signals\": signals}\n\ndef compute_arl(h, k, n_sim=3000, n_obs=2000, seed=0):\n    \"\"\"Monte Carlo ARL under in-control conditions (z ~ N(0,1)).\"\"\"\n    rng = np.random.default_rng(seed)\n    rls = []\n    for _ in range(n_sim):\n        z = rng.standard_normal(n_obs)\n        Sp = Sn = 0\n        for t, zt in enumerate(z):\n            Sp = max(0, Sp + zt - k); Sn = max(0, Sn - zt - k)\n            if Sp > h or Sn > h: rls.append(t); break\n        else: rls.append(n_obs)\n    return np.mean(rls)\n\nprint(\"CUSUM engine defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calibration \u2014 Threshold h and Cooldown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4a: Calibrate h to achieve target ARL ----\nK_OPT      = 0.50    # allowance: optimal for detecting ~1 sigma mean shifts\nTARGET_ARL = 500     # ~2 years between false alarms\n\nprint(f\"Calibrating h for ARL target = {TARGET_ARL} days  (k={K_OPT})\")\nprint(\"Running Monte Carlo simulation...\")\n\nh_grid = np.linspace(2.0, 8.0, 20)\narls = []\nfor h_val in h_grid:\n    a = compute_arl(h_val, K_OPT, n_sim=2000, n_obs=3000)\n    arls.append(a)\n    if a >= TARGET_ARL * 2.5: break\n\narls_arr = np.array(arls); h_grid_c = h_grid[:len(arls)]\nbest_idx = np.argmin(np.abs(arls_arr - TARGET_ARL))\nH_OPT = float(h_grid_c[best_idx])\nachieved = compute_arl(H_OPT, K_OPT, n_sim=3000)\nprint(f\"Optimal h = {H_OPT:.2f}  (achieved ARL = {achieved:.0f} days)\")\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot(h_grid_c, arls_arr, color=ACCENT, marker=\"o\", ms=5)\nax.axhline(TARGET_ARL, color=YELLOW, ls=\"--\", lw=1.5, label=f\"Target ARL={TARGET_ARL}\")\nax.axvline(H_OPT, color=RED, ls=\"--\", lw=1.5, label=f\"h={H_OPT:.2f}\")\nax.set_xlabel(\"Threshold h\"); ax.set_ylabel(\"Average Run Length (days)\")\nax.set_title(\"ARL Calibration Curve  (k=0.5)\"); ax.legend(); ax.grid(True, alpha=0.4)\nplt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4b: Calibrate cooldown from in-control inter-signal gap distribution ----\ndef measure_gaps(h, k, n_sim=400, n_obs=5000, seed=1):\n    rng = np.random.default_rng(seed); gaps = []\n    for _ in range(n_sim):\n        z = rng.standard_normal(n_obs)\n        res = run_cusum(z, h=h, k=k, cooldown=0)\n        idx = np.where(res[\"signals\"] != 0)[0]\n        if len(idx) > 1: gaps.extend(np.diff(idx).tolist())\n    return np.array(gaps)\n\nprint(\"Measuring inter-signal gap distribution on in-control data...\")\ngaps_ic = measure_gaps(H_OPT, K_OPT)\n\nif len(gaps_ic) > 10:\n    COOLDOWN = max(int(np.percentile(gaps_ic, 25)), 10)\n    print(f\"Gap stats: mean={gaps_ic.mean():.1f}d, P25={np.percentile(gaps_ic,25):.1f}d, \"\n          f\"P50={np.median(gaps_ic):.1f}d\")\n    fig, ax = plt.subplots(figsize=(10, 4))\n    ax.hist(gaps_ic, bins=40, color=PURPLE, alpha=0.7, edgecolor=\"none\")\n    ax.axvline(COOLDOWN, color=RED, ls=\"--\", lw=2, label=f\"Cooldown={COOLDOWN}d (P25)\")\n    ax.axvline(np.median(gaps_ic), color=YELLOW, ls=\":\", lw=1.5,\n               label=f\"Median={np.median(gaps_ic):.0f}d\")\n    ax.set_xlabel(\"Days Between Consecutive Signals\"); ax.set_ylabel(\"Frequency\")\n    ax.set_title(\"Inter-Signal Gap Distribution (In-Control)\"); ax.legend()\n    ax.grid(True, alpha=0.4); plt.tight_layout(); plt.show()\nelse:\n    COOLDOWN = 20\n    print(\"Few gaps found; defaulting cooldown=20\")\n\nprint(f\"\\nFinal parameters: k={K_OPT}, h={H_OPT:.2f}, cooldown={COOLDOWN} days\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run CUSUM on Full History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cusum_result = run_cusum(z_resid, h=H_OPT, k=K_OPT, cooldown=COOLDOWN)\nS_pos    = pd.Series(cusum_result[\"S_pos\"],   index=returns.index)\nS_neg    = pd.Series(cusum_result[\"S_neg\"],   index=returns.index)\nsignals  = pd.Series(cusum_result[\"signals\"], index=returns.index)\n\nup_signals   = signals[signals ==  1].index\ndown_signals = signals[signals == -1].index\nall_signals  = signals[signals !=  0].index\n\nprint(f\"Total signals : {(signals!=0).sum()}\")\nprint(f\"  Up   (bull) : {(signals==1).sum()}\")\nprint(f\"  Down (bear) : {(signals==-1).sum()}\")\nprint(\"\\nSignal dates:\")\nfor dt in all_signals:\n    d = signals[dt]\n    regime = true_regimes.loc[dt] if true_regimes is not None else \"N/A\"\n    print(f\"  {dt.date()}  {'UP   (+)' if d==1 else 'DOWN (-)'}  [{regime}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 14))\ngs  = gridspec.GridSpec(4, 1, hspace=0.06, height_ratios=[3,1.5,1.5,1.5])\nax1 = fig.add_subplot(gs[0])\nax2 = fig.add_subplot(gs[1], sharex=ax1)\nax3 = fig.add_subplot(gs[2], sharex=ax1)\nax4 = fig.add_subplot(gs[3], sharex=ax1)\n\n# Panel 1: Price + signal overlays\nax1.plot(price_index.index, price_index.values, color=\"#888\", lw=1.2, zorder=2)\nax1.set_ylabel(\"Price (log)\"); ax1.set_yscale(\"log\")\nax1.set_title(f\"CUSUM Regime Detection (h={H_OPT:.2f}, k={K_OPT}, cooldown={COOLDOWN}d)\", fontsize=13)\nax1.grid(True, alpha=0.3, zorder=1)\n\n# Shade alternating regimes by signal\nsig_list = [(dt, signals[dt]) for dt in all_signals]\nif sig_list:\n    cur_col = \"#ffffff08\"; prev_dt = returns.index[0]\n    for sig_dt, sig_dir in sig_list:\n        ax1.axvspan(prev_dt, sig_dt, color=cur_col, lw=0, zorder=0)\n        cur_col = \"#00d4aa18\" if sig_dir==1 else \"#ff4d6d18\"\n        prev_dt = sig_dt\n    ax1.axvspan(prev_dt, returns.index[-1], color=cur_col, lw=0, zorder=0)\n\nfor dt in up_signals:\n    ax1.axvline(dt, color=ACCENT, lw=1.2, alpha=0.9, zorder=3)\n    ax1.annotate(\"U\", xy=(dt, price_index.loc[dt]), color=ACCENT, fontsize=9,\n                 ha=\"center\", va=\"bottom\", xytext=(0,4), textcoords=\"offset points\")\nfor dt in down_signals:\n    ax1.axvline(dt, color=RED, lw=1.2, alpha=0.9, zorder=3)\n    ax1.annotate(\"D\", xy=(dt, price_index.loc[dt]), color=RED, fontsize=9,\n                 ha=\"center\", va=\"top\", xytext=(0,-4), textcoords=\"offset points\")\n\nlegend_elems = [\n    mpatches.Patch(color=ACCENT, alpha=0.7, label=\"Up signal (mean increase)\"),\n    mpatches.Patch(color=RED,    alpha=0.7, label=\"Down signal (mean decrease)\"),\n]\nax1.legend(handles=legend_elems, loc=\"upper left\")\n\n# Panel 2: GARCH residuals\nax2.fill_between(z_s.index, z_s, 0, where=z_s>=0, color=ACCENT, alpha=0.5, lw=0)\nax2.fill_between(z_s.index, z_s, 0, where=z_s<0,  color=RED,   alpha=0.5, lw=0)\nax2.axhline(0, color=\"white\", lw=0.5); ax2.set_ylabel(\"z (GARCH)\"); ax2.grid(True, alpha=0.3)\n\n# Panel 3: S+\nax3.plot(S_pos.index, S_pos.values, color=ACCENT, lw=1.2, label=\"S+ (up detector)\")\nax3.axhline(H_OPT, color=YELLOW, ls=\"--\", lw=1.5, label=f\"h={H_OPT:.2f}\")\nax3.fill_between(S_pos.index, S_pos.values, 0, alpha=0.2, color=ACCENT)\nax3.set_ylabel(\"S+\"); ax3.legend(loc=\"upper right\", fontsize=8); ax3.grid(True, alpha=0.3)\n\n# Panel 4: S-\nax4.plot(S_neg.index, S_neg.values, color=RED, lw=1.2, label=\"S- (down detector)\")\nax4.axhline(H_OPT, color=YELLOW, ls=\"--\", lw=1.5, label=f\"h={H_OPT:.2f}\")\nax4.fill_between(S_neg.index, S_neg.values, 0, alpha=0.2, color=RED)\nax4.set_ylabel(\"S-\"); ax4.legend(loc=\"upper right\", fontsize=8); ax4.grid(True, alpha=0.3)\n\nplt.setp(ax1.get_xticklabels(), visible=False)\nplt.setp(ax2.get_xticklabels(), visible=False)\nplt.setp(ax3.get_xticklabels(), visible=False)\nplt.savefig(\"cusum_dashboard.png\", dpi=150, bbox_inches=\"tight\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backtest\n\n**Strategy**: Regime-switching between full equity and cash.\n- Down signal \u2192 exit to cash (0% equity)\n- Up signal \u2192 re-enter market (100% equity)\n- Position executed at **next-day open** (1-day lag to avoid look-ahead bias)\n\nWe also test with a 10bps round-trip transaction cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(returns, signals, tc=0.0):\n    \"\"\"Regime-switching backtest. tc = one-way transaction cost (e.g. 0.001 = 10bps).\"\"\"\n    pos = 1.0; positions = np.ones(len(returns)); trades = 0\n    for t in range(len(returns)):\n        if   signals.iloc[t] == -1: pos = 0.0; trades += 1\n        elif signals.iloc[t] ==  1: pos = 1.0; trades += 1\n        positions[t] = pos\n    pos_s = pd.Series(positions, index=returns.index).shift(1).fillna(1.0)\n    tc_series = (signals != 0).astype(float) * tc\n    strat_rets = pos_s * returns - tc_series\n    return strat_rets, pos_s, trades\n\ndef perf_metrics(r, ann=252):\n    r = np.asarray(r)\n    ann_ret = r.mean() * ann; ann_vol = r.std() * np.sqrt(ann)\n    sharpe  = r.mean() / r.std() * np.sqrt(ann) if r.std() > 0 else 0\n    cum = np.exp(np.cumsum(r)); peak = np.maximum.accumulate(cum)\n    dd = (cum-peak)/peak; max_dd = dd.min()\n    calmar = ann_ret/abs(max_dd) if max_dd != 0 else np.nan\n    win_rate = (r > 0).mean()\n    return {\"Ann.Return\":ann_ret,\"Ann.Vol\":ann_vol,\"Sharpe\":sharpe,\n            \"MaxDD\":max_dd,\"Calmar\":calmar,\"WinRate\":win_rate}\n\nstrat_rets, position, n_trades   = backtest(returns, signals, tc=0.0)\nstrat_rets_tc, _, _              = backtest(returns, signals, tc=0.001)\nm_bh     = perf_metrics(returns.values)\nm_strat  = perf_metrics(strat_rets.values)\nm_strat_tc = perf_metrics(strat_rets_tc.values)\n\nprint(\"=== Performance Comparison ===\")\nhdr = f\"{'Strategy':<22} {'Ann.Ret':>9} {'Ann.Vol':>9} {'Sharpe':>8} {'MaxDD':>9} {'Calmar':>8} {'WinRate':>8}\"\nprint(hdr); print(\"-\"*75)\nfor name, m in [(\"Buy & Hold\",m_bh),(\"CUSUM (no TC)\",m_strat),(\"CUSUM (10bps TC)\",m_strat_tc)]:\n    print(f\"{name:<22} {m['Ann.Return']:>9.2%} {m['Ann.Vol']:>9.2%} \"\n          f\"{m['Sharpe']:>8.3f} {m['MaxDD']:>9.2%} {m['Calmar']:>8.3f} {m['WinRate']:>8.2%}\")\nprint(f\"\\nTotal trades executed: {n_trades}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal quality vs true bear regimes\nBEAR_REGIMES = {\"Dot-com bust\",\"GFC crash\",\"COVID crash\",\"Rate hike bear\"}\ntrue_bear     = true_regimes.isin(BEAR_REGIMES).astype(int)\ninferred_bear = (position < 0.5).astype(int)\ntb = true_bear.values; ib = inferred_bear.values\n\nTP = int(((ib==1)&(tb==1)).sum()); TN = int(((ib==0)&(tb==0)).sum())\nFP = int(((ib==1)&(tb==0)).sum()); FN = int(((ib==0)&(tb==1)).sum())\nprecision   = TP/(TP+FP+1e-9); recall    = TP/(TP+FN+1e-9)\nf1          = 2*precision*recall/(precision+recall+1e-9)\nspecificity = TN/(TN+FP+1e-9); accuracy  = (TP+TN)/len(tb)\n\nprint(\"=== Signal Quality vs True Regimes ===\")\nprint(f\"  Confusion matrix:  TP={TP:5d}  FN={FN:5d}\")\nprint(f\"                     FP={FP:5d}  TN={TN:5d}\")\nprint(f\"  Precision   : {precision:.3f}  (detected-bear days that were truly bear)\")\nprint(f\"  Recall      : {recall:.3f}  (true-bear days that were captured)\")\nprint(f\"  F1 Score    : {f1:.3f}\")\nprint(f\"  Specificity : {specificity:.3f}\")\nprint(f\"  Accuracy    : {accuracy:.3f}\")\n\nprint(\"\\n=== Signal Lag Analysis ===\")\nbear_starts = true_bear.index[true_bear.diff()==1]\nlags = []\nfor bs in bear_starts:\n    reg = true_regimes.loc[bs]\n    fd  = down_signals[down_signals >= bs]\n    if len(fd) > 0:\n        lag = (fd[0]-bs).days; lags.append(lag)\n        print(f\"  {reg:<22} {bs.date()} -> signal {fd[0].date()} (lag {lag}d)\")\n    else:\n        print(f\"  {reg:<22} {bs.date()} -> NO signal detected\")\nif lags:\n    print(f\"\\nMean detection lag: {np.mean(lags):.1f} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawdown(cum): peak = cum.cummax(); return (cum-peak)/peak\ncum_bh    = np.exp(returns.cumsum())\ncum_strat = np.exp(strat_rets.cumsum())\ndd_bh = drawdown(cum_bh); dd_strat = drawdown(cum_strat)\n\nfig, axes = plt.subplots(3,1,figsize=(16,12),sharex=True,\n                          gridspec_kw={\"height_ratios\":[3,1.5,1.5]})\n\nax = axes[0]\nax.plot(cum_bh.index, cum_bh.values, color=\"#888\", lw=1.2, label=\"Buy & Hold\")\nax.plot(cum_strat.index, cum_strat.values, color=ACCENT, lw=1.5, label=\"CUSUM Strategy\")\nax.set_yscale(\"log\"); ax.set_ylabel(\"Cumulative Return (log)\")\nax.set_title(\"Backtest: CUSUM Regime-Switching vs Buy & Hold\", fontsize=13)\nax.grid(True, alpha=0.3)\n# Shade bear periods\nin_bear=False; bear_start=None\nfor dt in position.index:\n    if position[dt]<0.5 and not in_bear: in_bear=True; bear_start=dt\n    elif position[dt]>=0.5 and in_bear:\n        ax.axvspan(bear_start,dt,color=\"#ff4d6d20\",lw=0); in_bear=False\nif in_bear: ax.axvspan(bear_start,position.index[-1],color=\"#ff4d6d20\",lw=0)\nfor dt in down_signals: ax.axvline(dt,color=RED,lw=0.8,alpha=0.6)\nfor dt in up_signals:   ax.axvline(dt,color=ACCENT,lw=0.8,alpha=0.6)\npatch = mpatches.Patch(color=\"#ff4d6d50\", label=\"CUSUM bear period\")\nax.legend(handles=[ax.get_lines()[0],ax.get_lines()[1],patch],loc=\"upper left\")\n\nax = axes[1]\nax.fill_between(dd_bh.index, dd_bh*100, 0, color=\"#666\", alpha=0.5, label=\"B&H\")\nax.fill_between(dd_strat.index, dd_strat*100, 0, color=ACCENT, alpha=0.5, label=\"CUSUM\")\nax.set_ylabel(\"Drawdown %\"); ax.set_title(\"Drawdown Comparison\")\nax.legend(loc=\"lower left\",fontsize=9); ax.grid(True, alpha=0.3)\n\nax = axes[2]\nax.fill_between(position.index, position.values, 0,\n    where=position.values>=0.5, color=ACCENT, alpha=0.6, label=\"Long (equity)\")\nax.fill_between(position.index, position.values, 0,\n    where=position.values<0.5, color=\"#555\", alpha=0.5, label=\"Cash\")\nax.set_ylim(-0.1,1.2); ax.set_ylabel(\"Position\"); ax.set_title(\"CUSUM-Derived Position\")\nax.legend(loc=\"lower right\",fontsize=9); ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\"backtest.png\", dpi=150, bbox_inches=\"tight\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1,figsize=(16,6),sharex=True)\nax = axes[0]\nax.fill_between(true_bear.index, true_bear.values, 0, color=RED, alpha=0.7, step=\"post\", label=\"Bear\")\nax.fill_between(true_bear.index, 1-true_bear.values, 0, color=ACCENT, alpha=0.4, step=\"post\", label=\"Bull\")\nax.set_ylabel(\"Regime\"); ax.set_title(\"True Regime Labels (from simulation)\")\nax.set_ylim(-0.05,1.1); ax.legend(loc=\"upper right\"); ax.grid(True, alpha=0.3)\n\nax = axes[1]\nax.fill_between(inferred_bear.index, inferred_bear.values, 0, color=RED, alpha=0.7, step=\"post\", label=\"CUSUM: Cash\")\nax.fill_between(inferred_bear.index, 1-inferred_bear.values, 0, color=ACCENT, alpha=0.4, step=\"post\", label=\"CUSUM: Long\")\nax.set_ylabel(\"Regime\"); ax.set_title(\"CUSUM-Inferred Regime\")\nax.set_ylim(-0.05,1.1); ax.legend(loc=\"upper right\"); ax.grid(True, alpha=0.3)\n\nplt.suptitle(f\"Regime Alignment  |  Precision={precision:.2f}  Recall={recall:.2f}  F1={f1:.2f}\", fontsize=12)\nplt.tight_layout()\nplt.savefig(\"regime_alignment.png\", dpi=150, bbox_inches=\"tight\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sensitivity Analysis \u2014 Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vals = np.linspace(2.0, 7.0, 6)\nk_vals = [0.25, 0.50, 0.75]\nresults = []\nfor h_v in h_vals:\n    for k_v in k_vals:\n        res   = run_cusum(z_resid, h=h_v, k=k_v, cooldown=COOLDOWN)\n        sigs  = pd.Series(res[\"signals\"], index=returns.index)\n        sr, pos, nt = backtest(returns, sigs)\n        ib_  = (pos<0.5).astype(int).values\n        TP_  = ((ib_==1)&(tb==1)).sum(); FP_ = ((ib_==1)&(tb==0)).sum()\n        FN_  = ((ib_==0)&(tb==1)).sum()\n        p_   = TP_/(TP_+FP_+1e-9); r_ = TP_/(TP_+FN_+1e-9)\n        f1_  = 2*p_*r_/(p_+r_+1e-9)\n        m    = perf_metrics(sr.values)\n        results.append({\"h\":h_v,\"k\":k_v,\"F1\":f1_,\"Sharpe\":m[\"Sharpe\"],\n                        \"MaxDD\":m[\"MaxDD\"],\"Signals\":int((sigs!=0).sum())})\n\nres_df = pd.DataFrame(results)\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\nfor ax, metric, cm in zip(axes,[\"F1\",\"Sharpe\",\"MaxDD\"],[\"viridis\",\"RdYlGn\",\"RdYlGn_r\"]):\n    pivot = res_df.pivot(index=\"h\", columns=\"k\", values=metric)\n    im = ax.imshow(pivot.values, aspect=\"auto\", cmap=cm, origin=\"lower\",\n                   extent=[k_vals[0]-0.13, k_vals[-1]+0.13, h_vals[0]-0.5, h_vals[-1]+0.5])\n    plt.colorbar(im, ax=ax)\n    for i, h_v in enumerate(pivot.index):\n        for j, k_v in enumerate(pivot.columns):\n            ax.text(k_v, h_v, f\"{pivot.iloc[i,j]:.2f}\", ha=\"center\", va=\"center\",\n                    color=\"white\", fontsize=8, fontweight=\"bold\")\n    ax.set_xlabel(\"k (allowance)\"); ax.set_ylabel(\"h (threshold)\")\n    ax.set_title(f\"{metric} vs (h, k)\"); ax.set_xticks(k_vals)\nplt.suptitle(\"Sensitivity Analysis\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"sensitivity.png\", dpi=150, bbox_inches=\"tight\"); plt.show()\n\nprint(\"Top 5 by F1:\")\nprint(res_df.sort_values(\"F1\",ascending=False).head(5).to_string(index=False))\nprint(\"\\nTop 5 by Sharpe:\")\nprint(res_df.sort_values(\"Sharpe\",ascending=False).head(5).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Rolling CUSUM \u2014 Adaptive Baseline\n\nRe-standardise GARCH residuals against a rolling 252-day window.\nThis makes the detector **local**: it asks whether the recent mean has shifted\nfrom the near-past. More robust to secular drift over multi-decade periods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 252\nz_roll_mean = z_s.rolling(WINDOW, min_periods=60).mean().fillna(0)\nz_roll_std  = z_s.rolling(WINDOW, min_periods=60).std().fillna(1).clip(lower=0.1)\nz_adaptive  = ((z_s - z_roll_mean) / z_roll_std).fillna(0)\n\nrc = run_cusum(z_adaptive.values, h=H_OPT, k=K_OPT, cooldown=COOLDOWN)\nrc_signals = pd.Series(rc[\"signals\"], index=returns.index)\nrc_sr, rc_pos, rc_nt = backtest(returns, rc_signals)\nm_rc = perf_metrics(rc_sr.values)\ncum_rc = np.exp(rc_sr.cumsum())\n\nfig, ax = plt.subplots(figsize=(16,5))\nax.plot(cum_bh.index,    cum_bh.values,    color=\"#666\",  lw=1.2, label=\"Buy & Hold\")\nax.plot(cum_strat.index, cum_strat.values, color=ACCENT,  lw=1.5, ls=\"--\", label=\"Global CUSUM\")\nax.plot(cum_rc.index,    cum_rc.values,    color=PURPLE,  lw=1.5, label=f\"Rolling CUSUM ({WINDOW}d)\")\nax.set_yscale(\"log\")\nrc_down = rc_signals[rc_signals==-1].index\nfor dt in rc_down: ax.axvline(dt, color=PURPLE, lw=0.7, alpha=0.5)\nax.set_title(f\"Rolling CUSUM vs Global CUSUM vs Buy & Hold\")\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(\"rolling_cusum.png\", dpi=150, bbox_inches=\"tight\"); plt.show()\n\nprint(f\"Rolling CUSUM: {(rc_signals!=0).sum()} signals, {rc_nt} trades\")\nprint(f\"  Sharpe: {m_rc['Sharpe']:.3f}  MaxDD: {m_rc['MaxDD']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*65)\nprint(\"  CUSUM REGIME DETECTION - FINAL SUMMARY\")\nprint(\"=\"*65)\nprint(f\"\\nDATA\")\nprint(f\"  Period         : {returns.index[0].date()} to {returns.index[-1].date()}\")\nprint(f\"  Observations   : {len(returns):,}\")\ngp = garch_params\nprint(f\"\\nGARCH(1,1)\")\nprint(f\"  alpha          : {gp['alpha']:.4f}  (ARCH effect)\")\nprint(f\"  beta           : {gp['beta']:.4f}  (GARCH persistence)\")\nprint(f\"  alpha+beta     : {gp['alpha']+gp['beta']:.4f}\")\nprint(f\"\\nCUSUM CALIBRATION\")\nprint(f\"  k (allowance)  : {K_OPT}\")\nprint(f\"  h (threshold)  : {H_OPT:.2f}  (target ARL = {TARGET_ARL} days)\")\nprint(f\"  cooldown       : {COOLDOWN} days\")\nprint(f\"\\nSIGNALS (Global CUSUM)\")\nprint(f\"  Total          : {(signals!=0).sum()}\")\nprint(f\"  Down (to cash) : {(signals==-1).sum()}\")\nprint(f\"  Up (to long)   : {(signals==1).sum()}\")\nprint(f\"\\nREGIME DETECTION QUALITY\")\nprint(f\"  Precision      : {precision:.3f}\")\nprint(f\"  Recall         : {recall:.3f}\")\nprint(f\"  F1 Score       : {f1:.3f}\")\nprint(f\"  Accuracy       : {accuracy:.3f}\")\nprint(f\"\\nBACKTEST PERFORMANCE\")\nhdr = f\"  {'Strategy':<20} {'Ann.Ret':>9} {'Vol':>9} {'Sharpe':>8} {'MaxDD':>9} {'Calmar':>8}\"\nprint(hdr); print(\"  \"+\"-\"*63)\nfor name, m in [(\"Buy & Hold\",m_bh),(\"Global CUSUM\",m_strat),(\"Rolling CUSUM\",m_rc)]:\n    print(f\"  {name:<20} {m['Ann.Return']:>9.2%} {m['Ann.Vol']:>9.2%} \"\n          f\"{m['Sharpe']:>8.3f} {m['MaxDD']:>9.2%} {m['Calmar']:>8.3f}\")\nprint(\"=\"*65)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Extensions & Real Data\n\n```python\n# --- Plug in real S&P 500 ---\n# pip install yfinance arch\nimport yfinance as yf\nfrom arch import arch_model\n\nraw     = yf.download(\"^GSPC\", start=\"2000-01-01\", auto_adjust=True)\nreturns = np.log(raw[\"Close\"]/raw[\"Close\"].shift(1)).dropna()\nam      = arch_model(returns, vol=\"Garch\", p=1, q=1, dist=\"t\")  # t-dist for fat tails\nres     = am.fit(disp=\"off\")\nz_resid = res.std_resid.values  # pass to run_cusum()\n\n# --- Multi-asset portfolio ---\n# Fit GARCH per asset -> z_i residuals\n# Compute: z_portfolio = weights @ Z_matrix  (weight-averaged)\n# Then: run_cusum(z_portfolio, ...)\n\n# --- Variance regime detection ---\n# Use squared residuals as CUSUM input to detect vol regime shifts\nz_var = z_resid**2 - 1\ncusum_vol = run_cusum(z_var, h=H_OPT, k=K_OPT, cooldown=COOLDOWN)\n\n# --- Add transaction costs ---\nstrat_rets_tc, _, _ = backtest(returns, signals, tc=0.001)  # 10bps per trade\n```\n\n| Design Choice | Rationale |\n|---|---|\n| GARCH(1,1) filtering | Removes vol clustering; prevents false alarms in turbulent periods |\n| k = 0.5 | Balanced detection of 1-sigma mean shifts |\n| ARL-calibrated h | Principled false-alarm control (not arbitrary) |\n| Cooldown = P25 gap | Blocks burst re-signalling; calibrated on in-control data |\n| Reset after signal | Prevents old accumulation contaminating new regime detection |\n| Rolling baseline option | Adapts to secular drift across multi-decade periods |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}