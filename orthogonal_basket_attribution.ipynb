{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rpjena/random_matrix/blob/main/orthogonal_basket_attribution.ipynb)\n",
    "\n",
    "# Portfolio PnL Attribution & Risk Decomposition Along Orthogonalized Stock Baskets\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Given a **long/short portfolio**, we want to understand its performance, PnL attribution, and risk exposure along **thousands of long-only stock baskets** (e.g., sector baskets, thematic baskets, style baskets).\n",
    "\n",
    "### The Problem with Raw Baskets\n",
    "\n",
    "Raw baskets are correlated with the broad market (e.g., S&P 500). A portfolio that appears to have large exposure to \"Tech\" may simply have market beta. To isolate *idiosyncratic* basket exposures, we **orthogonalize** each basket's returns against its corresponding market index.\n",
    "\n",
    "### Approach\n",
    "\n",
    "1. **Construct basket returns** from constituent weights\n",
    "2. **Orthogonalize** each basket against its market index via OLS regression, keeping the residual\n",
    "3. **Regress** the portfolio's daily returns on the orthogonalized basket returns to get exposures\n",
    "4. **Attribute PnL** = exposure x orthogonalized basket return (daily, cumulative)\n",
    "5. **Decompose risk** via covariance-based marginal/absolute contribution to risk\n",
    "6. **Present results** with clean formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "from scipy import linalg as sla\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "sns.set_theme(style='whitegrid', font_scale=1.1)\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulated Data\n",
    "\n",
    "We simulate:\n",
    "- A universe of **200 stocks** over **504 trading days** (~2 years)\n",
    "- **50 long-only baskets** (each containing a random subset of stocks)\n",
    "- Each basket is mapped to a **market index** (here we use a single S&P 500 proxy for simplicity; extend to multiple markets as needed)\n",
    "- A **long/short portfolio** with positions across these stocks\n",
    "\n",
    "Replace this section with your own data to use real baskets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "N_STOCKS = 200\n",
    "T_DAYS = 504\n",
    "N_BASKETS = 50\n",
    "BASKET_SIZE_RANGE = (10, 40)  # each basket has between 10 and 40 stocks\n",
    "ANNUAL_FACTOR = 252\n",
    "\n",
    "dates = pd.bdate_range('2023-01-02', periods=T_DAYS, freq='B')\n",
    "stock_names = [f'STOCK_{i:03d}' for i in range(N_STOCKS)]\n",
    "basket_names = [f'BASKET_{i:03d}' for i in range(N_BASKETS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simulate market index (S&P 500 proxy) ---\n",
    "market_daily_mu = 0.08 / ANNUAL_FACTOR\n",
    "market_daily_vol = 0.16 / np.sqrt(ANNUAL_FACTOR)\n",
    "market_returns = pd.Series(\n",
    "    np.random.normal(market_daily_mu, market_daily_vol, T_DAYS),\n",
    "    index=dates, name='SP500'\n",
    ")\n",
    "\n",
    "# --- Simulate individual stock returns ---\n",
    "# Each stock has a beta to the market plus idiosyncratic noise\n",
    "betas = np.random.uniform(0.5, 1.5, N_STOCKS)\n",
    "idio_vols = np.random.uniform(0.15, 0.45, N_STOCKS) / np.sqrt(ANNUAL_FACTOR)\n",
    "alphas = np.random.normal(0, 0.02 / ANNUAL_FACTOR, N_STOCKS)\n",
    "\n",
    "stock_returns = pd.DataFrame(index=dates, columns=stock_names, dtype=float)\n",
    "for i, s in enumerate(stock_names):\n",
    "    stock_returns[s] = (\n",
    "        alphas[i]\n",
    "        + betas[i] * market_returns.values\n",
    "        + np.random.normal(0, idio_vols[i], T_DAYS)\n",
    "    )\n",
    "\n",
    "print(f'Stock returns: {stock_returns.shape}')\n",
    "stock_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Construct long-only baskets with equal weights ---\n",
    "basket_weights = {}  # dict: basket_name -> Series(stock_name -> weight)\n",
    "basket_market_map = {}  # dict: basket_name -> market_index_name\n",
    "\n",
    "for b in basket_names:\n",
    "    size = np.random.randint(*BASKET_SIZE_RANGE)\n",
    "    members = np.random.choice(stock_names, size=size, replace=False)\n",
    "    w = pd.Series(1.0 / size, index=members, name=b)\n",
    "    basket_weights[b] = w\n",
    "    basket_market_map[b] = 'SP500'  # all US-based in this example\n",
    "\n",
    "# Compute basket returns\n",
    "basket_returns = pd.DataFrame(index=dates, columns=basket_names, dtype=float)\n",
    "for b in basket_names:\n",
    "    w = basket_weights[b]\n",
    "    basket_returns[b] = stock_returns[w.index].values @ w.values\n",
    "\n",
    "print(f'Basket returns: {basket_returns.shape}')\n",
    "basket_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simulate a long/short portfolio ---\n",
    "# Random positions: some long, some short, roughly dollar-neutral\n",
    "raw_positions = np.random.randn(N_STOCKS)\n",
    "raw_positions -= raw_positions.mean()  # roughly dollar-neutral\n",
    "portfolio_weights = pd.Series(raw_positions / np.abs(raw_positions).sum(),\n",
    "                              index=stock_names, name='portfolio')\n",
    "\n",
    "portfolio_returns = (stock_returns * portfolio_weights).sum(axis=1)\n",
    "portfolio_returns.name = 'Portfolio'\n",
    "\n",
    "print(f'Portfolio gross leverage: {portfolio_weights.abs().sum():.2f}')\n",
    "print(f'Portfolio net exposure:   {portfolio_weights.sum():.4f}')\n",
    "print(f'Portfolio ann. return:    {portfolio_returns.mean() * ANNUAL_FACTOR:.2%}')\n",
    "print(f'Portfolio ann. vol:       {portfolio_returns.std() * np.sqrt(ANNUAL_FACTOR):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Orthogonalize Baskets Against Their Market Index\n",
    "\n",
    "For each basket $b$ with market index $m_b$, we run:\n",
    "\n",
    "$$R^{\\text{basket}}_b(t) = \\alpha_b + \\beta_b \\cdot R^{\\text{market}}_{m_b}(t) + \\varepsilon_b(t)$$\n",
    "\n",
    "The **orthogonalized basket return** is the residual $\\varepsilon_b(t)$, which captures the basket's return *after removing market exposure*.\n",
    "\n",
    "We store the regression statistics (alpha, beta, R-squared) for diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonalize_baskets(basket_ret, market_ret_series, basket_market_map):\n",
    "    \"\"\"\n",
    "    Orthogonalize each basket's returns against its corresponding market index.\n",
    "\n",
    "    Parameters:\n",
    "        basket_ret (pd.DataFrame): T x B basket returns\n",
    "        market_ret_series (dict or pd.DataFrame): market returns keyed by market name.\n",
    "            If a single Series, wraps into a dict.\n",
    "        basket_market_map (dict): basket_name -> market_index_name\n",
    "\n",
    "    Returns:\n",
    "        ortho_ret (pd.DataFrame): T x B orthogonalized basket returns (residuals)\n",
    "        reg_stats (pd.DataFrame): B x 3 DataFrame with alpha, beta, R2 per basket\n",
    "    \"\"\"\n",
    "    if isinstance(market_ret_series, pd.Series):\n",
    "        market_ret_series = {market_ret_series.name: market_ret_series}\n",
    "    elif isinstance(market_ret_series, pd.DataFrame):\n",
    "        market_ret_series = {c: market_ret_series[c] for c in market_ret_series.columns}\n",
    "\n",
    "    ortho_ret = pd.DataFrame(index=basket_ret.index, columns=basket_ret.columns, dtype=float)\n",
    "    stats = []\n",
    "\n",
    "    for b in basket_ret.columns:\n",
    "        mkt_name = basket_market_map[b]\n",
    "        mkt = market_ret_series[mkt_name].reindex(basket_ret.index)\n",
    "        y = basket_ret[b].values\n",
    "        X = np.column_stack([np.ones(len(mkt)), mkt.values])\n",
    "\n",
    "        # OLS: y = X @ [alpha, beta] + eps\n",
    "        coeffs, residuals, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "        alpha_b, beta_b = coeffs\n",
    "        fitted = X @ coeffs\n",
    "        eps = y - fitted\n",
    "\n",
    "        ss_res = np.sum(eps ** 2)\n",
    "        ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "        r2 = 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "\n",
    "        ortho_ret[b] = eps\n",
    "        stats.append({'basket': b, 'market': mkt_name,\n",
    "                      'alpha_ann': alpha_b * ANNUAL_FACTOR,\n",
    "                      'beta': beta_b, 'R2': r2})\n",
    "\n",
    "    reg_stats = pd.DataFrame(stats).set_index('basket')\n",
    "    return ortho_ret, reg_stats\n",
    "\n",
    "\n",
    "ortho_basket_returns, regression_stats = orthogonalize_baskets(\n",
    "    basket_returns, market_returns, basket_market_map\n",
    ")\n",
    "\n",
    "print('=== Orthogonalization Regression Stats (first 10 baskets) ===')\n",
    "display(regression_stats.head(10).style\n",
    "    .format({'alpha_ann': '{:.4f}', 'beta': '{:.3f}', 'R2': '{:.3f}'})\n",
    "    .set_caption('Market Regression: alpha (annualized), beta, R²')\n",
    "    .background_gradient(subset=['R2'], cmap='YlOrRd')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify orthogonality: correlation of residuals with market should be ~0\n",
    "corr_with_market = ortho_basket_returns.corrwith(market_returns)\n",
    "print(f'Correlation of orthogonalized baskets with market:')\n",
    "print(f'  Mean:   {corr_with_market.mean():.6f}')\n",
    "print(f'  Max:    {corr_with_market.abs().max():.6f}')\n",
    "print(f'  Stdev:  {corr_with_market.std():.6f}')\n",
    "print('=> All near zero confirms successful orthogonalization.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Portfolio Exposure to Orthogonalized Baskets\n",
    "\n",
    "We regress the portfolio's returns on the orthogonalized basket returns using **ridge regression** (L2 regularization) since the number of baskets can be large and they may still be correlated with each other:\n",
    "\n",
    "$$R^{\\text{port}}(t) = \\sum_b \\delta_b \\cdot \\varepsilon_b(t) + \\eta(t)$$\n",
    "\n",
    "The coefficients $\\delta_b$ are the portfolio's exposures to each orthogonalized basket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_basket_exposures(port_ret, ortho_ret, ridge_lambda=0.01):\n",
    "    \"\"\"\n",
    "    Estimate portfolio exposures to orthogonalized baskets via ridge regression.\n",
    "\n",
    "    Parameters:\n",
    "        port_ret (pd.Series): T portfolio returns\n",
    "        ortho_ret (pd.DataFrame): T x B orthogonalized basket returns\n",
    "        ridge_lambda (float): L2 regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        exposures (pd.Series): B exposures (regression coefficients)\n",
    "        r2 (float): in-sample R-squared\n",
    "        residual (pd.Series): T unexplained returns\n",
    "    \"\"\"\n",
    "    X = ortho_ret.values  # T x B\n",
    "    y = port_ret.values   # T\n",
    "    B = X.shape[1]\n",
    "\n",
    "    # Ridge: (X'X + lambda*I)^-1 X'y\n",
    "    XtX = X.T @ X\n",
    "    Xty = X.T @ y\n",
    "    coeffs = np.linalg.solve(XtX + ridge_lambda * np.eye(B), Xty)\n",
    "\n",
    "    fitted = X @ coeffs\n",
    "    resid = y - fitted\n",
    "    ss_res = np.sum(resid ** 2)\n",
    "    ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "    r2 = 1.0 - ss_res / ss_tot\n",
    "\n",
    "    exposures = pd.Series(coeffs, index=ortho_ret.columns, name='exposure')\n",
    "    residual = pd.Series(resid, index=port_ret.index, name='unexplained')\n",
    "    return exposures, r2, residual\n",
    "\n",
    "\n",
    "exposures, model_r2, unexplained = estimate_basket_exposures(\n",
    "    portfolio_returns, ortho_basket_returns, ridge_lambda=0.01\n",
    ")\n",
    "\n",
    "print(f'Model R²: {model_r2:.4f}')\n",
    "print(f'\\nTop 10 basket exposures (by magnitude):')\n",
    "top_exp = exposures.abs().nlargest(10)\n",
    "display(exposures.loc[top_exp.index].to_frame('Exposure').style\n",
    "    .format('{:.4f}')\n",
    "    .bar(color=['#d65f5f', '#5fba7d'], align='mid')\n",
    "    .set_caption('Portfolio Exposure to Orthogonalized Baskets')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PnL Attribution\n",
    "\n",
    "Daily PnL attributed to basket $b$:\n",
    "\n",
    "$$\\text{PnL}_b(t) = \\delta_b \\cdot \\varepsilon_b(t)$$\n",
    "\n",
    "We compute daily and cumulative attribution, plus summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pnl_attribution(exposures, ortho_ret, port_ret):\n",
    "    \"\"\"\n",
    "    Compute PnL attribution of portfolio returns to orthogonalized baskets.\n",
    "\n",
    "    Parameters:\n",
    "        exposures (pd.Series): B exposures\n",
    "        ortho_ret (pd.DataFrame): T x B orthogonalized basket returns\n",
    "        port_ret (pd.Series): T portfolio returns\n",
    "\n",
    "    Returns:\n",
    "        daily_attr (pd.DataFrame): T x (B+1) daily PnL attribution (baskets + unexplained)\n",
    "        cum_attr (pd.DataFrame): T x (B+1) cumulative PnL attribution\n",
    "        summary (pd.DataFrame): B+1 summary stats\n",
    "    \"\"\"\n",
    "    # Daily attribution\n",
    "    daily_attr = ortho_ret.mul(exposures, axis=1)\n",
    "    daily_attr['Unexplained'] = port_ret.values - daily_attr.sum(axis=1).values\n",
    "\n",
    "    # Cumulative\n",
    "    cum_attr = daily_attr.cumsum()\n",
    "\n",
    "    # Summary statistics\n",
    "    total_pnl = daily_attr.sum()\n",
    "    ann_return = daily_attr.mean() * ANNUAL_FACTOR\n",
    "    ann_vol = daily_attr.std() * np.sqrt(ANNUAL_FACTOR)\n",
    "    sharpe = ann_return / ann_vol.replace(0, np.nan)\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Total PnL (cum ret)': total_pnl,\n",
    "        'Ann. Return': ann_return,\n",
    "        'Ann. Vol': ann_vol,\n",
    "        'Sharpe': sharpe,\n",
    "        'Pct of Total PnL': total_pnl / port_ret.sum() * 100\n",
    "    })\n",
    "\n",
    "    return daily_attr, cum_attr, summary\n",
    "\n",
    "\n",
    "daily_attr, cum_attr, attr_summary = compute_pnl_attribution(\n",
    "    exposures, ortho_basket_returns, portfolio_returns\n",
    ")\n",
    "\n",
    "# Show top contributors\n",
    "top_n = 15\n",
    "top_contributors = attr_summary['Total PnL (cum ret)'].abs().nlargest(top_n + 1).index\n",
    "print(f'=== Top {top_n} PnL Contributors (by absolute contribution) ===')\n",
    "display(attr_summary.loc[top_contributors].style\n",
    "    .format({\n",
    "        'Total PnL (cum ret)': '{:.4f}',\n",
    "        'Ann. Return': '{:.4f}',\n",
    "        'Ann. Vol': '{:.4f}',\n",
    "        'Sharpe': '{:.2f}',\n",
    "        'Pct of Total PnL': '{:.1f}%'\n",
    "    })\n",
    "    .background_gradient(subset=['Pct of Total PnL'], cmap='RdYlGn')\n",
    "    .bar(subset=['Total PnL (cum ret)'], color=['#d65f5f', '#5fba7d'], align='mid')\n",
    "    .set_caption('PnL Attribution Summary')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cumulative PnL Attribution Plot ---\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), gridspec_kw={'height_ratios': [2, 1]})\n",
    "\n",
    "# Top chart: cumulative PnL of top contributors\n",
    "ax = axes[0]\n",
    "top_baskets = attr_summary.drop('Unexplained', errors='ignore')[\n",
    "    'Total PnL (cum ret)'].abs().nlargest(8).index\n",
    "cum_attr[list(top_baskets)].plot(ax=ax, linewidth=1.5)\n",
    "cum_attr['Unexplained'].plot(ax=ax, linewidth=1.5, linestyle='--', color='grey', label='Unexplained')\n",
    "portfolio_returns.cumsum().plot(ax=ax, linewidth=2.5, color='black', label='Total Portfolio')\n",
    "ax.set_title('Cumulative PnL Attribution (Top 8 Baskets)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.legend(loc='upper left', fontsize=8, ncol=2)\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "# Bottom chart: stacked area of attribution\n",
    "ax2 = axes[1]\n",
    "# Use rolling 21-day sum for smoother visualization\n",
    "rolling_attr = daily_attr[list(top_baskets)].rolling(21).sum()\n",
    "rolling_attr.plot.area(ax=ax2, linewidth=0, alpha=0.7, stacked=True)\n",
    "ax2.set_title('Rolling 21-Day PnL Attribution (Top 8 Baskets)', fontsize=12)\n",
    "ax2.set_ylabel('21-Day Return')\n",
    "ax2.legend(loc='upper left', fontsize=7, ncol=2)\n",
    "ax2.axhline(0, color='black', linewidth=0.5)\n",
    "ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Risk Decomposition\n",
    "\n",
    "We decompose portfolio risk along orthogonalized baskets using **Marginal Contribution to Total Risk (MCTR)** and **Absolute Contribution to Total Risk (ACTR)**.\n",
    "\n",
    "Given exposures $\\delta$ and the covariance matrix $\\Sigma$ of orthogonalized basket returns:\n",
    "\n",
    "$$\\sigma_p = \\sqrt{\\delta^T \\Sigma \\delta}$$\n",
    "\n",
    "$$\\text{MCTR}_b = \\frac{(\\Sigma \\delta)_b}{\\sigma_p}, \\qquad \\text{ACTR}_b = \\delta_b \\cdot \\text{MCTR}_b$$\n",
    "\n",
    "ACTR sums to total portfolio variance: $\\sum_b \\text{ACTR}_b = \\sigma_p^2 / \\sigma_p = \\sigma_p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_decomposition(exposures, ortho_ret, annual_factor=252):\n",
    "    \"\"\"\n",
    "    Decompose portfolio risk along orthogonalized baskets.\n",
    "\n",
    "    Parameters:\n",
    "        exposures (pd.Series): B exposures\n",
    "        ortho_ret (pd.DataFrame): T x B orthogonalized basket returns\n",
    "        annual_factor (int): annualization factor\n",
    "\n",
    "    Returns:\n",
    "        risk_df (pd.DataFrame): risk decomposition per basket\n",
    "        portfolio_vol (float): annualized portfolio vol from this model\n",
    "    \"\"\"\n",
    "    cov_matrix = ortho_ret.cov() * annual_factor  # annualized\n",
    "    delta = exposures.values\n",
    "    cov = cov_matrix.values\n",
    "\n",
    "    port_var = delta @ cov @ delta\n",
    "    port_vol = np.sqrt(port_var)\n",
    "\n",
    "    mctr = (cov @ delta) / port_vol  # marginal contribution\n",
    "    actr = delta * mctr              # absolute contribution\n",
    "    pct_risk = actr / port_vol * 100 # percentage of total risk\n",
    "\n",
    "    risk_df = pd.DataFrame({\n",
    "        'Exposure': exposures.values,\n",
    "        'MCTR (ann)': mctr,\n",
    "        'ACTR (ann)': actr,\n",
    "        'Pct of Risk': pct_risk\n",
    "    }, index=exposures.index)\n",
    "\n",
    "    return risk_df, port_vol\n",
    "\n",
    "\n",
    "risk_decomp, model_port_vol = compute_risk_decomposition(\n",
    "    exposures, ortho_basket_returns\n",
    ")\n",
    "\n",
    "print(f'Annualized portfolio vol (basket model): {model_port_vol:.4f} ({model_port_vol:.2%})')\n",
    "print(f'Sum of ACTR (should equal port vol):     {risk_decomp[\"ACTR (ann)\"].sum():.4f}')\n",
    "print(f'\\n=== Top 15 Risk Contributors ===')\n",
    "\n",
    "top_risk = risk_decomp['ACTR (ann)'].abs().nlargest(15).index\n",
    "display(risk_decomp.loc[top_risk].style\n",
    "    .format({\n",
    "        'Exposure': '{:.4f}',\n",
    "        'MCTR (ann)': '{:.4f}',\n",
    "        'ACTR (ann)': '{:.4f}',\n",
    "        'Pct of Risk': '{:.1f}%'\n",
    "    })\n",
    "    .background_gradient(subset=['Pct of Risk'], cmap='RdYlGn_r')\n",
    "    .bar(subset=['ACTR (ann)'], color=['#d65f5f', '#5fba7d'], align='mid')\n",
    "    .set_caption('Risk Decomposition by Orthogonalized Basket')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Risk Decomposition Visualization ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart of top risk contributors\n",
    "ax = axes[0]\n",
    "top20_risk = risk_decomp['ACTR (ann)'].abs().nlargest(20)\n",
    "colors = ['#d65f5f' if v < 0 else '#5fba7d'\n",
    "          for v in risk_decomp.loc[top20_risk.index, 'ACTR (ann)']]\n",
    "risk_decomp.loc[top20_risk.index, 'ACTR (ann)'].plot.barh(ax=ax, color=colors)\n",
    "ax.set_title('Top 20 ACTR Contributors', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Annualized ACTR')\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Pie chart of absolute risk contribution\n",
    "ax2 = axes[1]\n",
    "top10_pct = risk_decomp['ACTR (ann)'].abs().nlargest(10)\n",
    "other_pct = risk_decomp['ACTR (ann)'].abs().sum() - top10_pct.sum()\n",
    "pie_data = pd.concat([top10_pct, pd.Series({'Other': other_pct})])\n",
    "pie_data.plot.pie(ax=ax2, autopct='%1.1f%%', startangle=90, fontsize=8)\n",
    "ax2.set_title('Risk Share (|ACTR|)', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combined Dashboard\n",
    "\n",
    "A single summary table combining PnL attribution and risk decomposition for the most important baskets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge attribution and risk into a single dashboard\n",
    "dashboard = attr_summary.join(risk_decomp[['MCTR (ann)', 'ACTR (ann)', 'Pct of Risk']])\n",
    "dashboard = dashboard.drop('Unexplained', errors='ignore')\n",
    "\n",
    "# Add basket beta from orthogonalization\n",
    "dashboard = dashboard.join(regression_stats[['beta', 'R2']])\n",
    "dashboard.columns = [\n",
    "    'Cum PnL', 'Ann Return', 'Ann Vol', 'Sharpe',\n",
    "    'PnL %', 'MCTR', 'ACTR', 'Risk %',\n",
    "    'Mkt Beta', 'Mkt R²'\n",
    "]\n",
    "\n",
    "# Sort by absolute PnL contribution\n",
    "dashboard = dashboard.reindex(\n",
    "    dashboard['Cum PnL'].abs().sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "print('=== Portfolio Attribution & Risk Dashboard (Top 20 Baskets) ===')\n",
    "display(dashboard.head(20).style\n",
    "    .format({\n",
    "        'Cum PnL': '{:.4f}',\n",
    "        'Ann Return': '{:.4f}',\n",
    "        'Ann Vol': '{:.4f}',\n",
    "        'Sharpe': '{:.2f}',\n",
    "        'PnL %': '{:.1f}%',\n",
    "        'MCTR': '{:.4f}',\n",
    "        'ACTR': '{:.4f}',\n",
    "        'Risk %': '{:.1f}%',\n",
    "        'Mkt Beta': '{:.2f}',\n",
    "        'Mkt R²': '{:.2f}'\n",
    "    })\n",
    "    .background_gradient(subset=['Sharpe'], cmap='RdYlGn', vmin=-2, vmax=2)\n",
    "    .background_gradient(subset=['Mkt R²'], cmap='YlOrRd')\n",
    "    .bar(subset=['Cum PnL'], color=['#d65f5f', '#5fba7d'], align='mid')\n",
    "    .bar(subset=['ACTR'], color=['#d65f5f', '#5fba7d'], align='mid')\n",
    "    .set_caption('Combined Attribution & Risk Dashboard')\n",
    "    .set_table_styles([{\n",
    "        'selector': 'th',\n",
    "        'props': [('background-color', '#2c3e50'), ('color', 'white'),\n",
    "                  ('font-size', '11px'), ('text-align', 'center')]\n",
    "    }])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Correlation heatmap of top orthogonalized baskets ---\n",
    "top_baskets_for_corr = dashboard.head(15).index\n",
    "corr_matrix = ortho_basket_returns[top_baskets_for_corr].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, vmin=-1, vmax=1, square=True, linewidths=0.5,\n",
    "            ax=ax, cbar_kws={'shrink': 0.8})\n",
    "ax.set_title('Correlation of Orthogonalized Basket Returns (Top 15)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scaling to 1000s of Baskets\n",
    "\n",
    "The code above works for any number of baskets. Key considerations when scaling:\n",
    "\n",
    "1. **Orthogonalization** is per-basket, so it scales linearly — O(B * T)\n",
    "2. **Ridge regression** avoids multicollinearity issues with many correlated baskets. Tune `ridge_lambda` via cross-validation if needed\n",
    "3. **Risk decomposition** requires a B x B covariance matrix. For B > 1000, consider:\n",
    "   - Shrinkage estimators (Ledoit-Wolf)\n",
    "   - Factor-based covariance estimation\n",
    "   - Sparse covariance (only keep significant correlations)\n",
    "\n",
    "Below we demonstrate with a larger basket count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Demonstrate scaling: 1000 baskets ---\n",
    "N_BASKETS_LARGE = 1000\n",
    "basket_names_lg = [f'B_{i:04d}' for i in range(N_BASKETS_LARGE)]\n",
    "basket_weights_lg = {}\n",
    "basket_market_map_lg = {}\n",
    "\n",
    "for b in basket_names_lg:\n",
    "    size = np.random.randint(10, 40)\n",
    "    members = np.random.choice(stock_names, size=size, replace=False)\n",
    "    basket_weights_lg[b] = pd.Series(1.0 / size, index=members)\n",
    "    basket_market_map_lg[b] = 'SP500'\n",
    "\n",
    "basket_ret_lg = pd.DataFrame(index=dates, columns=basket_names_lg, dtype=float)\n",
    "for b in basket_names_lg:\n",
    "    w = basket_weights_lg[b]\n",
    "    basket_ret_lg[b] = stock_returns[w.index].values @ w.values\n",
    "\n",
    "print(f'Large basket returns shape: {basket_ret_lg.shape}')\n",
    "\n",
    "ortho_lg, stats_lg = orthogonalize_baskets(basket_ret_lg, market_returns, basket_market_map_lg)\n",
    "exp_lg, r2_lg, resid_lg = estimate_basket_exposures(portfolio_returns, ortho_lg, ridge_lambda=1.0)\n",
    "\n",
    "print(f'Orthogonalization complete. R² of model: {r2_lg:.4f}')\n",
    "print(f'Non-trivial exposures (|exp| > 0.01): {(exp_lg.abs() > 0.01).sum()}')\n",
    "print(f'Top 10 exposures:')\n",
    "print(exp_lg.abs().nlargest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Distribution of basket betas and R² ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(stats_lg['beta'], bins=40, edgecolor='white', alpha=0.8, color='steelblue')\n",
    "axes[0].axvline(stats_lg['beta'].mean(), color='red', linestyle='--',\n",
    "                label=f'Mean={stats_lg[\"beta\"].mean():.2f}')\n",
    "axes[0].set_title('Distribution of Basket Market Betas', fontweight='bold')\n",
    "axes[0].set_xlabel('Beta to Market')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(stats_lg['R2'], bins=40, edgecolor='white', alpha=0.8, color='darkorange')\n",
    "axes[1].axvline(stats_lg['R2'].mean(), color='red', linestyle='--',\n",
    "                label=f'Mean={stats_lg[\"R2\"].mean():.2f}')\n",
    "axes[1].set_title('Distribution of Market R² per Basket', fontweight='bold')\n",
    "axes[1].set_xlabel('R²')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### What we built\n",
    "\n",
    "| Step | Method | Output |\n",
    "|------|--------|--------|\n",
    "| Orthogonalize baskets | OLS regression vs market index | Market-neutral basket returns |\n",
    "| Estimate exposures | Ridge regression | Portfolio loading on each basket |\n",
    "| PnL attribution | Exposure x orthogonalized return | Daily/cumulative PnL per basket |\n",
    "| Risk decomposition | Covariance-based MCTR/ACTR | Risk contribution per basket |\n",
    "\n",
    "### Key design choices\n",
    "\n",
    "- **Orthogonalization** removes market beta from each basket, isolating the idiosyncratic component\n",
    "- **Ridge regression** handles multicollinearity when the number of baskets is large\n",
    "- **MCTR/ACTR** provides additive risk decomposition that sums to total portfolio risk\n",
    "\n",
    "### To use with real data\n",
    "\n",
    "1. Replace `stock_returns` with actual daily stock returns (DataFrame: dates x tickers)\n",
    "2. Replace `basket_weights` with actual basket compositions (dict of Series)\n",
    "3. Replace `market_returns` with actual index returns (handle multiple markets via `basket_market_map`)\n",
    "4. Replace `portfolio_weights` / `portfolio_returns` with actual portfolio data\n",
    "5. Tune `ridge_lambda` — larger values shrink exposures toward zero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
