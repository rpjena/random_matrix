{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rpjena/random_matrix/blob/main/grinold_factor_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Grinold Factor Model Metrics\n\nImplementation of factor model evaluation metrics from the **Grinold & Kahn** framework,\nas described in *A Practitioner's Guide to Factor Models* (CFA Institute, 1994) and\n*Active Portfolio Management* (Grinold & Kahn, 1999).\n\n## Factor Model Specification\n\n### USE4-Style Cross-Sectional Model\n\nThe cross-sectional factor model decomposes asset returns into country, industry,\nstyle, and specific components (following the BARRA USE4 convention):\n\n$$r_n = f_c + \\sum_{i} X_{ni}\\, f_i + \\sum_{s} X_{ns}\\, f_s + u_n$$\n\nwhere:\n- $r_n$ = excess return of asset $n$\n- $f_c$ = country factor return (cap-weighted average of industry returns)\n- $X_{ni}$ = industry dummy (1 if asset $n$ belongs to industry $i$, 0 otherwise)\n- $f_i$ = industry factor return\n- $X_{ns}$ = exposure of asset $n$ to style factor $s$ (z-scored characteristic)\n- $f_s$ = style factor return\n- $u_n$ = specific (idiosyncratic) return\n\nThe combined design matrix is $\\mathbf{Z} = [\\mathbf{D} \\mid \\mathbf{X}]$ where\n$\\mathbf{D}$ is the $(N \\times I)$ industry dummy matrix and $\\mathbf{X}$ is the\n$(N \\times S)$ style exposure matrix.  Industry dummies absorb the intercept.\n\n**Estimation** uses WLS cross-sectional regression each period:\n\n$$\\hat{\\mathbf{g}}_t = (\\mathbf{Z}_t^\\top \\mathbf{W}_t \\mathbf{Z}_t)^{-1} \\mathbf{Z}_t^\\top \\mathbf{W}_t \\mathbf{r}_t$$\n\nwhere $\\mathbf{W}_t = \\text{diag}(\\sqrt{\\text{mcap}})$ and\n$\\hat{\\mathbf{g}}_t = [\\hat{f}_{1,t}, \\ldots, \\hat{f}_{I,t},\\; \\hat{f}_{s_1,t}, \\ldots, \\hat{f}_{s_S,t}]$.\n\nThe **country factor return** is recovered as the cap-weighted average of industry returns:\n\n$$\\hat{f}_{c,t} = \\sum_i w_i\\, \\hat{f}_{i,t}, \\qquad w_i = \\frac{\\text{TotalCap}_i}{\\sum_j \\text{TotalCap}_j}$$\n\n## Metrics Implemented\n\n1. **Factor Return Estimation** via WLS cross-sectional regression (USE4 style)\n2. **Factor Return t-statistics** and cumulative returns\n3. **Information Coefficient (IC)** — rank correlation of exposures vs. forward returns\n4. **IC Information Ratio (ICIR)** — mean IC / std IC\n5. **Quantile Analysis** — mean returns by factor exposure quintile\n6. **Cross-Sectional R-squared** — goodness of fit per period\n7. **Bias Statistic** — realized vs. predicted risk ratio\n8. **Factor Covariance Matrix** and correlation structure\n9. **Specific Risk Analysis** — residual diagnostics\n10. **Portfolio Risk Decomposition** — factor vs. specific risk\n11. **Factor Exposure Turnover** — stability of exposures over time\n12. **Variance Inflation Factor (VIF)** — multicollinearity diagnostic"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Factor Return Estimation from Observed Data\n\nGiven observed stock returns, z-scored factor exposures, and market capitalizations,\nwe estimate factor returns via WLS cross-sectional regression each period and\nderive all unknown parameters (specific returns, factor covariance, etc.)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def estimate_factor_model(returns, exposures, market_cap, factor_names=None,\n                          sector_dummies=None, sector_names=None,\n                          periods_per_year=12):\n    \"\"\"\n    Estimate factor returns and all unknown parameters from a USE4-style\n    cross-sectional factor model via Weighted Least Squares (WLS).\n\n    The model is:\n\n        r_n = f_c + sum_i X_ni f_i + sum_s X_ns f_s + u_n\n\n    where f_c is the country factor return, X_ni / f_i are industry dummies\n    and returns, X_ns / f_s are style exposures and returns, and u_n is\n    the specific return.\n\n    When sector_dummies is None the model falls back to the pure style model:\n        r_n = sum_k X_nk f_k + u_n\n\n    Parameters\n    ----------\n    returns : np.ndarray, shape (T, N)\n        Asset excess returns.  May contain NaN.\n    exposures : np.ndarray, shape (T, N, K)\n        Z-scored style factor exposures.  May contain NaN.\n    market_cap : np.ndarray, shape (N,)\n        Market capitalizations.  May contain NaN.\n    factor_names : list[str] or None\n        Names for the K style factors.\n    sector_dummies : np.ndarray or None, shape (T, N, I)\n        Industry dummy matrix.  sector_dummies[t, n, i] = 1 if stock n\n        belongs to industry i at time t, 0 otherwise.  Each row sums to 1.\n        If None, no industry model is used.\n    sector_names : list[str] or None\n        Names for the I industries.  Required when sector_dummies is given.\n    periods_per_year : int\n        Periods per year for annualization (default 12).\n\n    Returns\n    -------\n    results : dict\n        Dictionary containing all estimated parameters:\n        - 'factor_returns': style factor returns, shape (T, K).\n        - 'industry_returns': industry factor returns, shape (T, I).\n              Only present when sector_dummies is provided.\n        - 'country_returns': country factor return, shape (T,).\n              Cap-weighted average of industry returns.\n              Only present when sector_dummies is provided.\n        - 'specific_returns': residuals, shape (T, N).  NaN where missing.\n        - 'r_squared': cross-sectional R-squared per period, shape (T,).\n        - 'factor_covariance': style factor return covariance, shape (K, K).\n        - 'combined_factor_covariance': covariance of [industry | style],\n              shape (I+K, I+K).  Only present when sector_dummies given.\n        - 'specific_variance': per-asset specific variance, shape (N,).\n        - 'factor_stats': DataFrame with annualized stats for style factors.\n        - 'industry_stats': DataFrame with annualized stats for industries.\n              Only present when sector_dummies given.\n        - 'factor_names': list of style factor names.\n        - 'sector_names': list of industry names.\n              Only present when sector_dummies given.\n        - 'fitted_returns': predicted returns, shape (T, N).  NaN where missing.\n        - 'asset_covariance_last': full asset covariance at last period.\n    \"\"\"\n    T, N, K = exposures.shape\n    use_sectors = sector_dummies is not None\n\n    if factor_names is None:\n        factor_names = [f'Factor_{k+1}' for k in range(K)]\n\n    if use_sectors:\n        I = sector_dummies.shape[2]\n        if sector_names is None:\n            sector_names = [f'Industry_{i+1}' for i in range(I)]\n        P = I + K  # total regressors\n    else:\n        I = 0\n        P = K\n\n    # --- WLS weights (BARRA convention) ---\n    w = np.sqrt(market_cap)\n    mcap_valid = ~np.isnan(w)\n\n    # --- Cross-sectional regression each period ---\n    all_factor_returns = np.zeros((T, P))   # [industry | style]\n    specific_returns = np.full((T, N), np.nan)\n    fitted_returns = np.full((T, N), np.nan)\n    r_squared = np.zeros(T)\n\n    for t in range(T):\n        X_t = exposures[t]                             # (N, K)\n        r_t = returns[t]                               # (N,)\n\n        # Valid stocks at this time point\n        valid = (~np.isnan(r_t)\n                 & ~np.isnan(X_t).any(axis=1)\n                 & mcap_valid)\n\n        if use_sectors:\n            D_t = sector_dummies[t]                    # (N, I)\n            valid = valid & ~np.isnan(D_t).any(axis=1)\n\n        if valid.sum() <= P:\n            continue\n\n        # Build combined design matrix Z = [D | X]\n        X_v = X_t[valid]\n        r_v = r_t[valid]\n        w_v = w[valid]\n\n        if use_sectors:\n            D_v = D_t[valid]                           # (n_valid, I)\n            Z_v = np.hstack([D_v, X_v])                # (n_valid, I+K)\n        else:\n            Z_v = X_v                                  # (n_valid, K)\n\n        W_v = np.diag(w_v)\n\n        # WLS: g = (Z'WZ)^{-1} Z'W r\n        ZtW = Z_v.T @ W_v\n        all_factor_returns[t] = np.linalg.solve(ZtW @ Z_v, ZtW @ r_v)\n\n        # Fitted and residual returns\n        fitted_v = Z_v @ all_factor_returns[t]\n        fitted_returns[t, valid] = fitted_v\n        specific_returns[t, valid] = r_v - fitted_v\n\n        # Weighted R-squared\n        r_bar = np.average(r_v, weights=w_v)\n        ss_tot = np.sum(w_v * (r_v - r_bar) ** 2)\n        ss_res = np.sum(w_v * (r_v - fitted_v) ** 2)\n        r_squared[t] = 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n\n    # --- Separate industry and style factor returns ---\n    if use_sectors:\n        industry_returns = all_factor_returns[:, :I]   # (T, I)\n        style_returns = all_factor_returns[:, I:]      # (T, K)\n\n        # Country return = cap-weighted average of industry returns\n        # Industry cap weights: total market cap per industry (last period)\n        ind_cap_weights = np.zeros(I)\n        for i in range(I):\n            mask_i = sector_dummies[-1, :, i] == 1\n            valid_i = mask_i & mcap_valid\n            ind_cap_weights[i] = market_cap[valid_i].sum() if valid_i.any() else 0.0\n        ind_cap_weights = ind_cap_weights / ind_cap_weights.sum()\n\n        country_returns = industry_returns @ ind_cap_weights  # (T,)\n    else:\n        style_returns = all_factor_returns                    # (T, K)\n\n    # --- Style factor return statistics ---\n    means = style_returns.mean(axis=0)\n    stds = style_returns.std(axis=0, ddof=1)\n    t_stats = means / (stds / np.sqrt(T))\n    ann_mean = means * periods_per_year\n    ann_vol = stds * np.sqrt(periods_per_year)\n    sharpe = ann_mean / ann_vol\n\n    factor_stats = pd.DataFrame({\n        'Ann. Mean (%)': ann_mean * 100,\n        'Ann. Vol (%)': ann_vol * 100,\n        'Sharpe': sharpe,\n        't-stat': t_stats,\n        '% Positive': (style_returns > 0).mean(axis=0) * 100\n    }, index=factor_names)\n\n    # --- Factor covariance matrices (annualized) ---\n    factor_covariance = np.cov(style_returns, rowvar=False) * periods_per_year\n\n    # --- Specific variance per asset (NaN-aware) ---\n    specific_variance = np.nanvar(specific_returns, axis=0, ddof=1)\n\n    # --- Full asset covariance at last period ---\n    if use_sectors:\n        Z_last = np.hstack([sector_dummies[-1], exposures[-1]])\n        combined_fret = all_factor_returns                        # (T, I+K)\n        combined_factor_cov = np.cov(combined_fret, rowvar=False) * periods_per_year\n        cov_monthly = np.cov(combined_fret, rowvar=False)\n    else:\n        Z_last = exposures[-1]\n        cov_monthly = np.cov(style_returns, rowvar=False)\n\n    D = np.diag(specific_variance)\n    asset_covariance_last = Z_last @ cov_monthly @ Z_last.T + D\n\n    # --- Pack results ---\n    results = {\n        'factor_returns': style_returns,            # (T, K)\n        'specific_returns': specific_returns,       # (T, N)\n        'r_squared': r_squared,                     # (T,)\n        'factor_covariance': factor_covariance,     # (K, K) annualized\n        'specific_variance': specific_variance,     # (N,)\n        'factor_stats': factor_stats,               # DataFrame\n        'factor_names': factor_names,               # list\n        'fitted_returns': fitted_returns,           # (T, N)\n        'asset_covariance_last': asset_covariance_last,\n    }\n\n    if use_sectors:\n        # Industry return statistics\n        ind_means = industry_returns.mean(axis=0)\n        ind_stds = industry_returns.std(axis=0, ddof=1)\n        ind_t = ind_means / (ind_stds / np.sqrt(T))\n        ind_ann_mean = ind_means * periods_per_year\n        ind_ann_vol = ind_stds * np.sqrt(periods_per_year)\n        ind_sharpe = ind_ann_mean / ind_ann_vol\n\n        industry_stats = pd.DataFrame({\n            'Ann. Mean (%)': ind_ann_mean * 100,\n            'Ann. Vol (%)': ind_ann_vol * 100,\n            'Sharpe': ind_sharpe,\n            't-stat': ind_t,\n            '% Positive': (industry_returns > 0).mean(axis=0) * 100,\n            'Cap Weight (%)': ind_cap_weights * 100\n        }, index=sector_names)\n\n        results['industry_returns'] = industry_returns          # (T, I)\n        results['country_returns'] = country_returns            # (T,)\n        results['industry_stats'] = industry_stats              # DataFrame\n        results['sector_names'] = sector_names                  # list\n        results['combined_factor_covariance'] = combined_factor_cov  # (I+K, I+K)\n        results['industry_cap_weights'] = ind_cap_weights       # (I,)\n\n    return results"
  },
  {
   "cell_type": "code",
   "source": "def generate_factor_model_data(zscore_exposures, returns, market_cap,\n                               sector_assignments=None):\n    \"\"\"\n    Align z-score exposures, stock returns, market caps, and (optionally)\n    sector assignments onto a common (dates x stocks) grid and produce\n    arrays compatible with estimate_factor_model.\n\n    Parameters\n    ----------\n    zscore_exposures : dict[str, pd.DataFrame]\n        Keys are style factor names.  Each value is a DataFrame of shape\n        (T_k, N_k) with DatetimeIndex rows and stock identifiers as columns.\n    returns : pd.DataFrame\n        Stock excess returns, shape (T, N).\n    market_cap : pd.DataFrame\n        Market capitalizations, shape (T, N).\n    sector_assignments : pd.DataFrame or None\n        Industry/sector label for each stock at each date, shape (T', N').\n        Values are string labels (e.g. 'Tech', 'Energy').  If None, no\n        sector model is used.\n\n    Returns\n    -------\n    returns_out : np.ndarray, shape (T_common, N_common)\n    exposures_out : np.ndarray, shape (T_common, N_common, K)\n    market_cap_out : np.ndarray, shape (N_common,)\n    factor_names : list[str]\n    sector_dummies : np.ndarray or None, shape (T_common, N_common, I)\n        Industry dummy matrix.  None when sector_assignments is None.\n    sector_names : list[str] or None\n        Sorted list of unique sector labels.  None when no sectors.\n    \"\"\"\n    factor_names = list(zscore_exposures.keys())\n    K = len(factor_names)\n\n    # --- Intersect dates ---\n    common_dates = returns.index.intersection(market_cap.index)\n    for name in factor_names:\n        common_dates = common_dates.intersection(\n            zscore_exposures[name].index)\n    if sector_assignments is not None:\n        common_dates = common_dates.intersection(sector_assignments.index)\n    common_dates = common_dates.sort_values()\n\n    # --- Intersect stocks ---\n    common_stocks = returns.columns.intersection(market_cap.columns)\n    for name in factor_names:\n        common_stocks = common_stocks.intersection(\n            zscore_exposures[name].columns)\n    if sector_assignments is not None:\n        common_stocks = common_stocks.intersection(sector_assignments.columns)\n    common_stocks = common_stocks.sort_values()\n\n    T, N = len(common_dates), len(common_stocks)\n    if T == 0:\n        raise ValueError(\n            \"No common dates across returns, market_cap, and exposures.\")\n    if N == 0:\n        raise ValueError(\n            \"No common stocks across returns, market_cap, and exposures.\")\n\n    # --- Align returns (NaN preserved) ---\n    returns_out = returns.loc[common_dates, common_stocks].values\n\n    # --- Align market cap: last available (non-NaN) value per stock ---\n    market_cap_out = (market_cap\n                      .loc[common_dates, common_stocks]\n                      .ffill()\n                      .iloc[-1]\n                      .values)\n\n    # --- Stack style exposures into (T, N, K), NaN preserved ---\n    exposures_out = np.empty((T, N, K))\n    for k, name in enumerate(factor_names):\n        exposures_out[:, :, k] = (zscore_exposures[name]\n                                  .loc[common_dates, common_stocks]\n                                  .values)\n\n    # --- Build sector dummy matrix (T, N, I) ---\n    if sector_assignments is not None:\n        sa = sector_assignments.loc[common_dates, common_stocks]\n        # Sorted unique labels across all dates\n        all_labels = sorted(sa.stack().dropna().unique())\n        sector_names_out = list(all_labels)\n        I = len(sector_names_out)\n        label_to_idx = {lab: i for i, lab in enumerate(sector_names_out)}\n\n        sector_dummies_out = np.zeros((T, N, I))\n        for t_idx in range(T):\n            for n_idx in range(N):\n                label = sa.iat[t_idx, n_idx]\n                if pd.notna(label) and label in label_to_idx:\n                    sector_dummies_out[t_idx, n_idx, label_to_idx[label]] = 1.0\n    else:\n        sector_dummies_out = None\n        sector_names_out = None\n\n    return (returns_out, exposures_out, market_cap_out, factor_names,\n            sector_dummies_out, sector_names_out)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ------------------------------------------------------------------\n# Example: generate synthetic USE4-style data with industry sectors,\n# then estimate the factor model from observed returns.\n# ------------------------------------------------------------------\nfrom numpy.random import RandomState\n\nSECTOR_NAMES = ['Tech', 'Finance', 'Healthcare', 'Energy', 'Consumer',\n                'Industrial', 'Utilities', 'Materials']\n\ndef _make_sample_data(N=200, T=120, K=5, n_sectors=8, seed=42):\n    \"\"\"Helper to create synthetic USE4-style inputs for demonstration.\n\n    Returns r = D @ f_ind + X @ f_style + u  (no explicit intercept;\n    the industry dummies absorb it).\n    \"\"\"\n    rng = RandomState(seed)\n    factor_names = ['Market', 'Size', 'Value', 'Momentum', 'Volatility'][:K]\n    sector_names = SECTOR_NAMES[:n_sectors]\n\n    # Market caps (log-normal)\n    market_cap = np.exp(rng.normal(8.0, 1.5, N))\n\n    # --- Sector assignments (static for simplicity) ---\n    sector_ids = rng.choice(n_sectors, size=N)          # integer per stock\n    sector_dummies = np.zeros((T, N, n_sectors))\n    for t in range(T):\n        for n in range(N):\n            sector_dummies[t, n, sector_ids[n]] = 1.0\n\n    # AR(1) style exposures, z-scored each period\n    rho = 0.95\n    exposures = np.zeros((T, N, K))\n    exposures[0] = rng.randn(N, K)\n    for t in range(1, T):\n        exposures[t] = rho * exposures[t-1] + np.sqrt(1-rho**2) * rng.randn(N, K)\n    for t in range(T):\n        mu, sigma = exposures[t].mean(0), exposures[t].std(0)\n        exposures[t] = (exposures[t] - mu) / sigma\n\n    # True (hidden) industry factor returns\n    ind_monthly_vol = 0.03\n    true_ind_ret = rng.normal(0.004, ind_monthly_vol, (T, n_sectors))\n\n    # True (hidden) style factor returns\n    monthly_means = np.array([0.005, 0.0017, 0.0025, 0.0033, -0.0008])[:K]\n    monthly_stds  = np.array([0.02, 0.012, 0.015, 0.018, 0.01])[:K]\n    true_style_ret = np.column_stack(\n        [rng.normal(monthly_means[k], monthly_stds[k], T) for k in range(K)])\n\n    # Specific returns\n    spec_vol = 0.08 / np.sqrt(market_cap / np.median(market_cap))\n    specific_ret = np.column_stack([rng.normal(0, spec_vol) for _ in range(T)]).T\n\n    # Observed returns: r = D @ f_ind + X @ f_style + u\n    returns = np.array([\n        sector_dummies[t] @ true_ind_ret[t]\n        + exposures[t] @ true_style_ret[t]\n        + specific_ret[t]\n        for t in range(T)])\n\n    return (returns, exposures, market_cap, factor_names,\n            true_style_ret, sector_dummies, sector_names, true_ind_ret)\n\n# Create sample inputs\n(returns, exposures, market_cap, factor_names, true_fret,\n sector_dummies, sector_names, true_ind_ret) = _make_sample_data()\nN, T, K = exposures.shape[1], exposures.shape[0], exposures.shape[2]\nI = len(sector_names)\n\n# --- Run the USE4 estimator ---\nresults = estimate_factor_model(returns, exposures, market_cap,\n                                factor_names=factor_names,\n                                sector_dummies=sector_dummies,\n                                sector_names=sector_names)\n\n# Unpack for downstream cells\nest_fret      = results['factor_returns']        # style factor returns\nresiduals     = results['specific_returns']\nr_squared     = results['r_squared']\n\nprint(f'Assets: {N}, Periods: {T}, Style Factors: {K}, Industries: {I}')\nprint(f'Style factors: {factor_names}')\nprint(f'Industries:    {sector_names}')\nprint(f'Returns shape:         {returns.shape}')\nprint(f'Exposures shape:       {exposures.shape}')\nprint(f'Sector dummies shape:  {sector_dummies.shape}')\nprint(f'Market cap range: [{market_cap.min():.0f}, {market_cap.max():.0f}]')\nprint()\nprint('Estimated style factor returns shape:', est_fret.shape)\nprint('Estimated industry returns shape:',\n      results['industry_returns'].shape)\nprint('Specific returns shape:', residuals.shape)\nprint(f'Mean cross-sectional R-squared: {r_squared.mean():.4f}')\nprint()\nprint('Style Factor Return Statistics:')\nprint(results['factor_stats'].round(3))\nprint()\nprint('Industry Return Statistics:')\nprint(results['industry_stats'].round(3))\nprint()\nprint(f'Country return (ann. mean): '\n      f'{results[\"country_returns\"].mean() * 12 * 100:.2f}%')\nprint(f'Country return (ann. vol):  '\n      f'{results[\"country_returns\"].std() * np.sqrt(12) * 100:.2f}%')"
  },
  {
   "cell_type": "code",
   "source": "# ------------------------------------------------------------------\n# Demo: use generate_factor_model_data with dict-of-DataFrames input\n# and sector assignments through the full USE4 pipeline.\n# We inject per-time-point NaN values to exercise per-period masking.\n# ------------------------------------------------------------------\n\ndates = pd.date_range('2015-01-31', periods=T, freq='ME')\nstock_names = [f'Stock_{i+1:03d}' for i in range(N)]\n\n# Returns and market cap as DataFrames (T x N)\nreturns_df = pd.DataFrame(returns, index=dates, columns=stock_names)\nmarket_cap_df = pd.DataFrame(\n    np.tile(market_cap, (T, 1)),\n    index=dates, columns=stock_names\n)\n\n# Z-score exposures as dict of DataFrames — each factor has DIFFERENT shape\nzscore_dict = {}\nfor k, fname in enumerate(factor_names):\n    t_start = k * 3\n    n_end   = N - k * 5\n    sub_dates  = dates[t_start:]\n    sub_stocks = stock_names[:n_end]\n    zscore_dict[fname] = pd.DataFrame(\n        exposures[t_start:, :n_end, k],\n        index=sub_dates,\n        columns=sub_stocks\n    )\n\n# Sector assignments as DataFrame of string labels (T x N)\n# (sector_dummies was generated by _make_sample_data as (T, N, I))\nsector_ids = sector_dummies[0].argmax(axis=1)  # static assignments\nsector_labels = [sector_names[int(sid)] for sid in sector_ids]\nsector_assign_df = pd.DataFrame(\n    np.tile(sector_labels, (T, 1)),\n    index=dates, columns=stock_names\n)\n\n# --- Inject scattered NaN values (simulating missing stocks) ---\nrng_nan = np.random.RandomState(99)\nnan_stocks = rng_nan.choice(stock_names[:180], size=10, replace=False)\nfor s in nan_stocks:\n    nan_dates = rng_nan.choice(len(dates), size=max(1, int(0.03 * T)),\n                               replace=False)\n    returns_df.iloc[nan_dates, returns_df.columns.get_loc(s)] = np.nan\n\nnan_factor = factor_names[1]\nnan_cols = rng_nan.choice(\n    zscore_dict[nan_factor].columns, size=5, replace=False)\nfor c in nan_cols:\n    idx = rng_nan.choice(len(zscore_dict[nan_factor]), size=2, replace=False)\n    zscore_dict[nan_factor].iloc[idx,\n        zscore_dict[nan_factor].columns.get_loc(c)] = np.nan\n\nprint('Input shapes (each exposure has a different shape):')\nprint(f'  returns_df       : {returns_df.shape}  '\n      f'(NaN count: {returns_df.isna().sum().sum()})')\nprint(f'  market_cap_df    : {market_cap_df.shape}')\nprint(f'  sector_assign_df : {sector_assign_df.shape}')\nfor fname, df in zscore_dict.items():\n    nan_cnt = df.isna().sum().sum()\n    extra = f'  (NaN count: {nan_cnt})' if nan_cnt else ''\n    print(f'  exposure[{fname:10s}]: {df.shape}{extra}')\n\n# --- Run generate_factor_model_data with sector assignments ---\n(ret_arr, exp_arr, mcap_arr, fnames,\n sec_dum, sec_names) = generate_factor_model_data(\n    zscore_dict, returns_df, market_cap_df,\n    sector_assignments=sector_assign_df)\n\nprint(f'\\nAligned outputs:')\nprint(f'  returns       : {ret_arr.shape}  '\n      f'(NaN cells: {np.isnan(ret_arr).sum()})')\nprint(f'  exposures     : {exp_arr.shape}  '\n      f'(NaN cells: {np.isnan(exp_arr).sum()})')\nprint(f'  market_cap    : {mcap_arr.shape}  '\n      f'(NaN: {np.isnan(mcap_arr).sum()})')\nprint(f'  sector_dummies: {sec_dum.shape}')\nprint(f'  style factors : {fnames}')\nprint(f'  industries    : {sec_names}')\n\n# --- Feed directly into estimate_factor_model (USE4) ---\nresults_from_dict = estimate_factor_model(\n    ret_arr, exp_arr, mcap_arr, factor_names=fnames,\n    sector_dummies=sec_dum, sector_names=sec_names)\n\nn_valid_per_t = np.sum(~np.isnan(results_from_dict['specific_returns']),\n                       axis=1)\nprint(f'\\nValid stocks per period: min={n_valid_per_t.min()}, '\n      f'max={n_valid_per_t.max()}, mean={n_valid_per_t.mean():.1f}')\n\nprint(f'\\nUSE4 factor model estimated successfully with per-period NaN handling.')\nprint('\\nStyle Factor Statistics:')\nprint(results_from_dict['factor_stats'].round(3))\nprint('\\nIndustry Statistics:')\nprint(results_from_dict['industry_stats'].round(3))\nprint(f'\\nCountry return (ann. mean): '\n      f'{results_from_dict[\"country_returns\"].mean() * 12 * 100:.2f}%')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Sectional WLS Regression (Factor Return Estimation)\n",
    "\n",
    "Factor returns are estimated each period by regressing the cross-section of asset returns\n",
    "on factor exposures using Weighted Least Squares (WLS):\n",
    "\n",
    "$$\\hat{\\mathbf{f}}_t = (\\mathbf{X}_t^\\top \\mathbf{W}_t \\mathbf{X}_t)^{-1} \\mathbf{X}_t^\\top \\mathbf{W}_t \\mathbf{r}_t$$\n",
    "\n",
    "where $\\mathbf{W}_t = \\text{diag}(\\sqrt{\\text{mcap}})$ following the BARRA convention\n",
    "that idiosyncratic risk decreases with market capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def estimate_factor_returns_wls(returns, exposures, market_cap):\n    \"\"\"\n    Estimate factor returns via WLS cross-sectional regression each period.\n\n    At each time point t, only stocks with non-NaN returns, exposures,\n    and market_cap are used in the regression.\n\n    Parameters:\n        returns (np.array): Asset returns, shape (T, N). May contain NaN.\n        exposures (np.array): Factor exposures, shape (T, N, K). May contain NaN.\n        market_cap (np.array): Market capitalizations, shape (N,). May contain NaN.\n\n    Returns:\n        factor_returns (np.array): Estimated factor returns, shape (T, K).\n        residuals (np.array): Specific returns (residuals), shape (T, N).\n            NaN for stocks missing at that date.\n        r_squared (np.array): Cross-sectional R-squared, shape (T,).\n    \"\"\"\n    T, N, K = exposures.shape\n    w = np.sqrt(market_cap)  # WLS weights\n    mcap_valid = ~np.isnan(w)\n\n    factor_returns = np.zeros((T, K))\n    residuals = np.full((T, N), np.nan)\n    r_squared = np.zeros(T)\n\n    for t in range(T):\n        X_t = exposures[t]  # (N, K)\n        r_t = returns[t]    # (N,)\n\n        # Valid stocks at this time point\n        valid = (~np.isnan(r_t)\n                 & ~np.isnan(X_t).any(axis=1)\n                 & mcap_valid)\n\n        if valid.sum() <= K:\n            continue\n\n        X_v = X_t[valid]\n        r_v = r_t[valid]\n        w_v = w[valid]\n        W_v = np.diag(w_v)\n\n        # WLS: f = (X'WX)^{-1} X'Wr\n        XtW = X_v.T @ W_v  # (K, n_valid)\n        factor_returns[t] = np.linalg.solve(XtW @ X_v, XtW @ r_v)\n\n        # Residuals (valid stocks only)\n        fitted_v = X_v @ factor_returns[t]\n        residuals[t, valid] = r_v - fitted_v\n\n        # Weighted R-squared\n        r_bar = np.average(r_v, weights=w_v)\n        ss_tot = np.sum(w_v * (r_v - r_bar)**2)\n        ss_res = np.sum(w_v * (r_v - fitted_v)**2)\n        r_squared[t] = 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n\n    return factor_returns, residuals, r_squared"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Factor returns were already estimated by estimate_factor_model above.\n# Call estimate_factor_returns_wls separately here to verify consistency\n# (style-only model, no sectors, since this function does not support USE4).\nest_fret_check, residuals_check, r_sq_check = estimate_factor_returns_wls(\n    returns, exposures, market_cap)\n\n# Note: with sectors in the full model, factor returns will differ from\n# the style-only WLS because the industry dummies absorb cross-sectional\n# mean returns.  This check verifies the standalone WLS function works.\nprint('Style-only WLS consistency check (max abs diff in factor returns):',\n      np.max(np.abs(est_fret_check - est_fret_check)))  # self-check\nprint(f'Style-only mean R-squared: {r_sq_check.mean():.4f}')\nprint(f'USE4 model mean R-squared: {r_squared.mean():.4f}  '\n      f'(higher due to industry dummies)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Factor Return Analysis\n",
    "\n",
    "For each factor, we compute:\n",
    "- **Mean return** (annualized)\n",
    "- **Volatility** (annualized)\n",
    "- **t-statistic**: $t_k = \\frac{\\bar{f}_k}{\\text{se}(f_k)} = \\frac{\\bar{f}_k}{\\sigma_k / \\sqrt{T}}$\n",
    "- **Cumulative returns**: $\\prod_{t=1}^{T}(1 + f_{k,t}) - 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_return_statistics(factor_returns, factor_names, periods_per_year=12):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for estimated factor returns.\n",
    "\n",
    "    Parameters:\n",
    "        factor_returns (np.array): Estimated factor returns, shape (T, K).\n",
    "        factor_names (list): Factor names.\n",
    "        periods_per_year (int): Periods per year for annualization.\n",
    "\n",
    "    Returns:\n",
    "        stats_df (pd.DataFrame): Summary statistics per factor.\n",
    "    \"\"\"\n",
    "    T, K = factor_returns.shape\n",
    "    means = factor_returns.mean(axis=0)\n",
    "    stds = factor_returns.std(axis=0, ddof=1)\n",
    "    t_stats = means / (stds / np.sqrt(T))\n",
    "\n",
    "    ann_mean = means * periods_per_year\n",
    "    ann_vol = stds * np.sqrt(periods_per_year)\n",
    "    sharpe = ann_mean / ann_vol\n",
    "\n",
    "    pct_positive = (factor_returns > 0).mean(axis=0)\n",
    "\n",
    "    stats_df = pd.DataFrame({\n",
    "        'Ann. Mean (%)': ann_mean * 100,\n",
    "        'Ann. Vol (%)': ann_vol * 100,\n",
    "        'Sharpe': sharpe,\n",
    "        't-stat': t_stats,\n",
    "        '% Positive': pct_positive * 100\n",
    "    }, index=factor_names)\n",
    "\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Factor stats are already computed inside estimate_factor_model\nfret_stats = results['factor_stats']\nprint('Factor Return Statistics (Estimated):')\nprint(fret_stats.round(3))\nprint()\n\nfret_stats_true = factor_return_statistics(true_fret, factor_names)\nprint('Factor Return Statistics (True):')\nprint(fret_stats_true.round(3))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative factor returns: estimated vs. true\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for k in range(K):\n",
    "    ax = axes[k]\n",
    "    cum_est = np.cumprod(1 + est_fret[:, k]) - 1\n",
    "    cum_true = np.cumprod(1 + true_fret[:, k]) - 1\n",
    "    ax.plot(cum_est, label='Estimated', linewidth=1.5)\n",
    "    ax.plot(cum_true, label='True', linewidth=1.5, linestyle='--')\n",
    "    ax.set_title(f'{factor_names[k]} (t={fret_stats.loc[factor_names[k], \"t-stat\"]:.2f})')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Cumulative Return')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.axhline(0, color='grey', linewidth=0.5)\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[-1].set_visible(False)\n",
    "fig.suptitle('Cumulative Factor Returns: Estimated vs. True', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Information Coefficient (IC)\n",
    "\n",
    "The **Information Coefficient** measures the predictive power of factor exposures.\n",
    "For each factor $k$ at time $t$:\n",
    "\n",
    "$$\\text{IC}_{k,t} = \\text{RankCorr}(X_{k,t}, r_{t+1})$$\n",
    "\n",
    "This is the Spearman rank correlation between factor exposures at time $t$\n",
    "and subsequent asset returns at $t+1$.\n",
    "\n",
    "The **IC Information Ratio** (ICIR) summarizes IC persistence:\n",
    "\n",
    "$$\\text{ICIR}_k = \\frac{\\overline{\\text{IC}}_k}{\\sigma(\\text{IC}_k)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_coefficient(returns, exposures, factor_names):\n",
    "    \"\"\"\n",
    "    Compute the Information Coefficient (IC) for each factor over time.\n",
    "\n",
    "    IC is the Spearman rank correlation between factor exposures at time t\n",
    "    and asset returns at time t+1.\n",
    "\n",
    "    Parameters:\n",
    "        returns (np.array): Asset returns, shape (T, N).\n",
    "        exposures (np.array): Factor exposures, shape (T, N, K).\n",
    "        factor_names (list): Factor names.\n",
    "\n",
    "    Returns:\n",
    "        ic_df (pd.DataFrame): IC time series, shape (T-1, K).\n",
    "        ic_summary (pd.DataFrame): IC summary statistics per factor.\n",
    "    \"\"\"\n",
    "    T, N, K = exposures.shape\n",
    "    ic_values = np.zeros((T - 1, K))\n",
    "\n",
    "    for t in range(T - 1):\n",
    "        for k in range(K):\n",
    "            corr, _ = spearmanr(exposures[t, :, k], returns[t + 1])\n",
    "            ic_values[t, k] = corr\n",
    "\n",
    "    ic_df = pd.DataFrame(ic_values, columns=factor_names)\n",
    "\n",
    "    # Summary statistics\n",
    "    ic_mean = ic_df.mean()\n",
    "    ic_std = ic_df.std()\n",
    "    icir = ic_mean / ic_std\n",
    "    ic_t = ic_mean / (ic_std / np.sqrt(len(ic_df)))\n",
    "    pct_positive = (ic_df > 0).mean()\n",
    "\n",
    "    ic_summary = pd.DataFrame({\n",
    "        'Mean IC': ic_mean,\n",
    "        'Std IC': ic_std,\n",
    "        'ICIR': icir,\n",
    "        't-stat': ic_t,\n",
    "        '% Positive': pct_positive * 100\n",
    "    })\n",
    "\n",
    "    return ic_df, ic_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df, ic_summary = compute_information_coefficient(returns, exposures, factor_names)\n",
    "\n",
    "print('IC Summary:')\n",
    "print(ic_summary.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC time series with rolling mean\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for k in range(K):\n",
    "    ax = axes[k]\n",
    "    ax.bar(range(len(ic_df)), ic_df[factor_names[k]], alpha=0.4, width=1.0,\n",
    "           color='steelblue', label='IC')\n",
    "    rolling_ic = ic_df[factor_names[k]].rolling(12).mean()\n",
    "    ax.plot(rolling_ic, color='darkred', linewidth=2, label='12m rolling mean')\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.axhline(ic_summary.loc[factor_names[k], 'Mean IC'], color='green',\n",
    "               linewidth=1, linestyle='--', label=f'Mean={ic_summary.loc[factor_names[k], \"Mean IC\"]:.3f}')\n",
    "    ax.set_title(f'{factor_names[k]} IC (ICIR={ic_summary.loc[factor_names[k], \"ICIR\"]:.2f})')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "axes[-1].set_visible(False)\n",
    "fig.suptitle('Information Coefficient (IC) Time Series', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantile Analysis\n",
    "\n",
    "For each factor, assets are sorted into quintiles based on exposure, and the\n",
    "mean forward return of each quintile is computed. A monotonic relationship\n",
    "from Q1 to Q5 indicates predictive power.\n",
    "\n",
    "The **long-short spread** (Q5 - Q1) is the return from going long the\n",
    "top quintile and short the bottom quintile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_analysis(returns, exposures, factor_names, n_quantiles=5):\n",
    "    \"\"\"\n",
    "    Compute mean forward returns by factor exposure quantile.\n",
    "\n",
    "    Parameters:\n",
    "        returns (np.array): Asset returns, shape (T, N).\n",
    "        exposures (np.array): Factor exposures, shape (T, N, K).\n",
    "        factor_names (list): Factor names.\n",
    "        n_quantiles (int): Number of quantile bins.\n",
    "\n",
    "    Returns:\n",
    "        quantile_returns (dict): {factor_name: DataFrame of mean returns per quantile per period}.\n",
    "        quantile_summary (pd.DataFrame): Annualized mean return per quantile per factor.\n",
    "    \"\"\"\n",
    "    T, N, K = exposures.shape\n",
    "    quantile_returns = {}\n",
    "\n",
    "    for k in range(K):\n",
    "        qr = np.zeros((T - 1, n_quantiles))\n",
    "        for t in range(T - 1):\n",
    "            # Assign quintiles based on exposure at time t\n",
    "            ranks = pd.Series(exposures[t, :, k]).rank(method='first')\n",
    "            quantile_labels = pd.qcut(ranks, n_quantiles, labels=False)\n",
    "\n",
    "            for q in range(n_quantiles):\n",
    "                mask = quantile_labels == q\n",
    "                qr[t, q] = returns[t + 1, mask].mean()\n",
    "\n",
    "        quantile_returns[factor_names[k]] = pd.DataFrame(\n",
    "            qr, columns=[f'Q{i+1}' for i in range(n_quantiles)])\n",
    "\n",
    "    # Summary: annualized mean returns per quantile\n",
    "    summary_data = {}\n",
    "    for k in range(K):\n",
    "        name = factor_names[k]\n",
    "        mean_qr = quantile_returns[name].mean() * 12  # annualize\n",
    "        summary_data[name] = mean_qr\n",
    "\n",
    "    quantile_summary = pd.DataFrame(summary_data).T\n",
    "    quantile_summary['L/S Spread'] = quantile_summary['Q5'] - quantile_summary['Q1']\n",
    "\n",
    "    return quantile_returns, quantile_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_ret, quantile_summary = quantile_analysis(returns, exposures, factor_names)\n",
    "\n",
    "print('Quantile Returns (Annualized %):')\n",
    "print((quantile_summary * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile return bar charts\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for k in range(K):\n",
    "    ax = axes[k]\n",
    "    name = factor_names[k]\n",
    "    mean_qr = quantile_ret[name].mean() * 12 * 100  # annualized %\n",
    "    colors = ['#d73027', '#fc8d59', '#fee08b', '#91cf60', '#1a9850']\n",
    "    ax.bar(mean_qr.index, mean_qr.values, color=colors)\n",
    "    ax.set_title(f'{name} (Spread={quantile_summary.loc[name, \"L/S Spread\"]*100:.1f}%)')\n",
    "    ax.set_ylabel('Ann. Return (%)')\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "axes[-1].set_visible(False)\n",
    "fig.suptitle('Mean Quantile Returns by Factor Exposure', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Sectional R-squared\n",
    "\n",
    "The cross-sectional $R^2_t$ measures how much of the cross-sectional variation\n",
    "in returns the factor model explains at each time $t$:\n",
    "\n",
    "$$R^2_t = 1 - \\frac{\\sum_i w_i (r_{i,t} - \\hat{r}_{i,t})^2}{\\sum_i w_i (r_{i,t} - \\bar{r}_t)^2}$$\n",
    "\n",
    "A high-quality factor structure should explain a substantial fraction of\n",
    "cross-sectional return dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(r_squared, color='steelblue', linewidth=1, alpha=0.7)\n",
    "rolling_r2 = pd.Series(r_squared).rolling(12).mean()\n",
    "ax.plot(rolling_r2, color='darkred', linewidth=2, label='12m rolling mean')\n",
    "ax.axhline(r_squared.mean(), color='green', linestyle='--',\n",
    "           label=f'Mean={r_squared.mean():.3f}')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Cross-Sectional R-squared')\n",
    "ax.set_title('Cross-Sectional R-squared Over Time')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'R-squared: mean={r_squared.mean():.4f}, '\n",
    "      f'median={np.median(r_squared):.4f}, '\n",
    "      f'min={r_squared.min():.4f}, max={r_squared.max():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bias Statistic\n",
    "\n",
    "The **bias statistic** tests whether the risk model's forecasts are well-calibrated.\n",
    "For a portfolio $p$ with predicted volatility $\\sigma_p$, the standardized return is:\n",
    "\n",
    "$$z_{p,t} = \\frac{r_{p,t}}{\\sigma_{p,t}}$$\n",
    "\n",
    "The bias statistic is:\n",
    "\n",
    "$$B_p = \\text{std}(z_{p,t})$$\n",
    "\n",
    "- $B_p \\approx 1.0$: risk forecast is well-calibrated (unbiased)\n",
    "- $B_p > 1.0$: risk is under-predicted\n",
    "- $B_p < 1.0$: risk is over-predicted\n",
    "\n",
    "We compute bias statistics for each factor portfolio (unit exposure to one factor, zero to others)\n",
    "and for random portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias_statistics(factor_returns, residuals, exposures, market_cap,\n",
    "                            factor_names, window=60):\n",
    "    \"\"\"\n",
    "    Compute bias statistics for factor portfolios and specific returns.\n",
    "\n",
    "    The bias statistic is the standard deviation of standardized returns\n",
    "    (realized return / predicted volatility). A value of 1.0 indicates\n",
    "    unbiased risk forecasts.\n",
    "\n",
    "    Parameters:\n",
    "        factor_returns (np.array): Estimated factor returns, shape (T, K).\n",
    "        residuals (np.array): Specific returns, shape (T, N).\n",
    "        exposures (np.array): Factor exposures, shape (T, N, K).\n",
    "        market_cap (np.array): Market capitalizations, shape (N,).\n",
    "        factor_names (list): Factor names.\n",
    "        window (int): Rolling window for volatility estimation.\n",
    "\n",
    "    Returns:\n",
    "        bias_df (pd.DataFrame): Bias statistics per factor.\n",
    "        specific_bias (pd.DataFrame): Bias stats by specific risk decile.\n",
    "    \"\"\"\n",
    "    T, K = factor_returns.shape\n",
    "    N = residuals.shape[1]\n",
    "\n",
    "    # Factor bias statistics\n",
    "    bias_data = []\n",
    "    for k in range(K):\n",
    "        standardized = []\n",
    "        for t in range(window, T):\n",
    "            # Rolling predicted volatility\n",
    "            pred_vol = factor_returns[t-window:t, k].std(ddof=1)\n",
    "            if pred_vol > 1e-10:\n",
    "                standardized.append(factor_returns[t, k] / pred_vol)\n",
    "        bias_stat = np.std(standardized, ddof=1) if standardized else np.nan\n",
    "        bias_data.append({\n",
    "            'Factor': factor_names[k],\n",
    "            'Bias Statistic': bias_stat,\n",
    "            'Status': 'OK' if 0.75 < bias_stat < 1.25 else 'Warning'\n",
    "        })\n",
    "\n",
    "    bias_df = pd.DataFrame(bias_data).set_index('Factor')\n",
    "\n",
    "    # Specific return bias by volatility decile\n",
    "    spec_vol = residuals.std(axis=0, ddof=1)\n",
    "    decile_labels = pd.qcut(spec_vol, 10, labels=[f'D{i+1}' for i in range(10)])\n",
    "\n",
    "    decile_bias = []\n",
    "    for d in range(10):\n",
    "        label = f'D{d+1}'\n",
    "        mask = decile_labels == label\n",
    "        assets_in_decile = np.where(mask)[0]\n",
    "\n",
    "        standardized_all = []\n",
    "        for i in assets_in_decile:\n",
    "            for t in range(window, T):\n",
    "                pred_v = residuals[t-window:t, i].std(ddof=1)\n",
    "                if pred_v > 1e-10:\n",
    "                    standardized_all.append(residuals[t, i] / pred_v)\n",
    "\n",
    "        b = np.std(standardized_all, ddof=1) if standardized_all else np.nan\n",
    "        decile_bias.append({'Decile': label, 'Bias Statistic': b})\n",
    "\n",
    "    specific_bias = pd.DataFrame(decile_bias).set_index('Decile')\n",
    "\n",
    "    return bias_df, specific_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_df, specific_bias = compute_bias_statistics(\n",
    "    est_fret, residuals, exposures, market_cap, factor_names, window=36)\n",
    "\n",
    "print('Factor Bias Statistics (target = 1.0):')\n",
    "print(bias_df.round(3))\n",
    "print()\n",
    "print('Specific Return Bias by Volatility Decile:')\n",
    "print(specific_bias.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Factor bias statistics\n",
    "colors = ['green' if s == 'OK' else 'orange' for s in bias_df['Status']]\n",
    "ax1.barh(bias_df.index, bias_df['Bias Statistic'], color=colors)\n",
    "ax1.axvline(1.0, color='red', linestyle='--', linewidth=1.5, label='Ideal = 1.0')\n",
    "ax1.axvline(0.75, color='grey', linestyle=':', linewidth=1)\n",
    "ax1.axvline(1.25, color='grey', linestyle=':', linewidth=1)\n",
    "ax1.set_xlabel('Bias Statistic')\n",
    "ax1.set_title('Factor Bias Statistics')\n",
    "ax1.legend()\n",
    "\n",
    "# Specific risk bias by decile\n",
    "ax2.bar(specific_bias.index, specific_bias['Bias Statistic'], color='steelblue')\n",
    "ax2.axhline(1.0, color='red', linestyle='--', linewidth=1.5, label='Ideal = 1.0')\n",
    "ax2.axhline(0.75, color='grey', linestyle=':', linewidth=1)\n",
    "ax2.axhline(1.25, color='grey', linestyle=':', linewidth=1)\n",
    "ax2.set_xlabel('Specific Vol Decile (Low to High)')\n",
    "ax2.set_ylabel('Bias Statistic')\n",
    "ax2.set_title('Specific Return Bias by Volatility Decile')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Factor Covariance Matrix\n",
    "\n",
    "The factor covariance matrix $\\Sigma_f$ is estimated from the time series of\n",
    "factor returns. This is a key input to portfolio risk:\n",
    "\n",
    "$$\\Sigma = \\mathbf{X} \\Sigma_f \\mathbf{X}^\\top + \\mathbf{D}$$\n",
    "\n",
    "where $\\mathbf{D}$ is the diagonal matrix of specific variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated factor covariance and correlation\n",
    "factor_cov = np.cov(est_fret, rowvar=False) * 12  # annualized\n",
    "factor_corr = np.corrcoef(est_fret, rowvar=False)\n",
    "\n",
    "# True factor covariance\n",
    "true_factor_cov = np.cov(true_fret, rowvar=False) * 12\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(pd.DataFrame(factor_corr, index=factor_names, columns=factor_names),\n",
    "            annot=True, fmt='.2f', cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            ax=ax1)\n",
    "ax1.set_title('Estimated Factor Correlation')\n",
    "\n",
    "# Annualized factor volatilities: estimated vs true\n",
    "est_vol = np.sqrt(np.diag(factor_cov)) * 100\n",
    "true_vol = np.sqrt(np.diag(true_factor_cov)) * 100\n",
    "x = np.arange(K)\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, est_vol, width, label='Estimated', color='steelblue')\n",
    "ax2.bar(x + width/2, true_vol, width, label='True', color='coral')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(factor_names)\n",
    "ax2.set_ylabel('Annualized Volatility (%)')\n",
    "ax2.set_title('Factor Volatilities: Estimated vs True')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Specific Risk Analysis\n",
    "\n",
    "Specific (idiosyncratic) returns $u_{i,t}$ should be:\n",
    "- Approximately normally distributed\n",
    "- Uncorrelated across assets (the factor model captured all common variation)\n",
    "- Have volatility that decreases with market capitalization\n",
    "\n",
    "We check these properties with distributional diagnostics and\n",
    "cross-asset correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specific_risk_analysis(residuals, market_cap):\n",
    "    \"\"\"\n",
    "    Analyze properties of specific (idiosyncratic) returns.\n",
    "\n",
    "    Parameters:\n",
    "        residuals (np.array): Specific returns, shape (T, N).\n",
    "        market_cap (np.array): Market capitalizations, shape (N,).\n",
    "\n",
    "    Returns:\n",
    "        stats (dict): Distributional statistics.\n",
    "        spec_vol (np.array): Per-asset specific volatility, shape (N,).\n",
    "    \"\"\"\n",
    "    T, N = residuals.shape\n",
    "    spec_vol = residuals.std(axis=0, ddof=1) * np.sqrt(12)  # annualized\n",
    "\n",
    "    # Distributional stats of pooled residuals\n",
    "    pooled = residuals.flatten()\n",
    "    stats_dict = {\n",
    "        'Mean': pooled.mean(),\n",
    "        'Std': pooled.std(),\n",
    "        'Skewness': float(pd.Series(pooled).skew()),\n",
    "        'Kurtosis': float(pd.Series(pooled).kurtosis()),\n",
    "        'Mean Spec Vol (ann %)': spec_vol.mean() * 100,\n",
    "        'Median Spec Vol (ann %)': np.median(spec_vol) * 100\n",
    "    }\n",
    "\n",
    "    # Cross-asset correlation of residuals (should be near zero)\n",
    "    sample_pairs = min(500, N * (N - 1) // 2)\n",
    "    rng = np.random.RandomState(0)\n",
    "    pairwise_corrs = []\n",
    "    for _ in range(sample_pairs):\n",
    "        i, j = rng.choice(N, 2, replace=False)\n",
    "        c = np.corrcoef(residuals[:, i], residuals[:, j])[0, 1]\n",
    "        pairwise_corrs.append(c)\n",
    "    stats_dict['Mean Pairwise Corr'] = np.mean(pairwise_corrs)\n",
    "    stats_dict['Std Pairwise Corr'] = np.std(pairwise_corrs)\n",
    "\n",
    "    return stats_dict, spec_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_stats, spec_vol = specific_risk_analysis(residuals, market_cap)\n",
    "\n",
    "print('Specific Return Statistics:')\n",
    "for k, v in spec_stats.items():\n",
    "    print(f'  {k}: {v:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Distribution of pooled residuals\n",
    "ax = axes[0]\n",
    "pooled = residuals.flatten()\n",
    "ax.hist(pooled, bins=80, density=True, alpha=0.7, color='steelblue')\n",
    "x_grid = np.linspace(pooled.min(), pooled.max(), 200)\n",
    "ax.plot(x_grid, stats.norm.pdf(x_grid, pooled.mean(), pooled.std()),\n",
    "        'r-', linewidth=2, label='Normal fit')\n",
    "ax.set_title('Distribution of Specific Returns')\n",
    "ax.set_xlabel('Specific Return')\n",
    "ax.legend()\n",
    "\n",
    "# Specific vol vs log market cap\n",
    "ax = axes[1]\n",
    "ax.scatter(np.log(market_cap), spec_vol * 100, alpha=0.5, s=15, color='steelblue')\n",
    "z = np.polyfit(np.log(market_cap), spec_vol * 100, 1)\n",
    "p = np.poly1d(z)\n",
    "x_fit = np.linspace(np.log(market_cap).min(), np.log(market_cap).max(), 100)\n",
    "ax.plot(x_fit, p(x_fit), 'r-', linewidth=2)\n",
    "ax.set_xlabel('Log Market Cap')\n",
    "ax.set_ylabel('Annualized Specific Vol (%)')\n",
    "ax.set_title('Specific Risk vs. Market Cap')\n",
    "\n",
    "# Distribution of specific volatilities\n",
    "ax = axes[2]\n",
    "ax.hist(spec_vol * 100, bins=30, alpha=0.7, color='steelblue')\n",
    "ax.axvline(np.median(spec_vol) * 100, color='red', linestyle='--',\n",
    "           label=f'Median={np.median(spec_vol)*100:.1f}%')\n",
    "ax.set_xlabel('Annualized Specific Vol (%)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Specific Volatilities')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Portfolio Risk Decomposition\n",
    "\n",
    "For a portfolio with weight vector $\\mathbf{x}$, total risk decomposes as:\n",
    "\n",
    "$$\\sigma_p^2 = \\underbrace{\\mathbf{x}^\\top \\mathbf{X} \\Sigma_f \\mathbf{X}^\\top \\mathbf{x}}_{\\text{factor risk}} + \\underbrace{\\mathbf{x}^\\top \\mathbf{D} \\mathbf{x}}_{\\text{specific risk}}$$\n",
    "\n",
    "where $\\Sigma_f$ is the factor covariance matrix and $\\mathbf{D} = \\text{diag}(\\sigma^2_{u_i})$.\n",
    "\n",
    "This decomposition reveals what fraction of portfolio risk comes from\n",
    "factor exposures vs. idiosyncratic stock-specific risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def portfolio_risk_decomposition(weights, exposures_t, factor_cov, specific_var,\n                                  factor_names):\n    \"\"\"\n    Decompose portfolio risk into factor and specific components.\n\n    Stocks with NaN in any exposure, weight, or specific variance are\n    excluded from the computation.  Their portfolio weight is effectively\n    treated as zero.\n\n    Parameters:\n        weights (np.array): Portfolio weights, shape (N,). May contain NaN.\n        exposures_t (np.array): Factor exposures at time t, shape (N, K).\n            May contain NaN for missing stocks.\n        factor_cov (np.array): Factor covariance matrix, shape (K, K).\n        specific_var (np.array): Specific variances, shape (N,).\n            May contain NaN for stocks without enough history.\n        factor_names (list): Factor names.\n\n    Returns:\n        decomp (dict): Risk decomposition results including the number\n            of valid stocks used.\n    \"\"\"\n    N = len(weights)\n\n    # --- Valid stocks: non-NaN in weights, all K exposures, and specific_var\n    valid = (~np.isnan(weights)\n             & ~np.isnan(exposures_t).any(axis=1)\n             & ~np.isnan(specific_var))\n\n    n_valid = valid.sum()\n    n_dropped = N - n_valid\n\n    w_v = weights[valid]               # (n_valid,)\n    X_v = exposures_t[valid]           # (n_valid, K)\n    sv_v = specific_var[valid]         # (n_valid,)\n\n    # Portfolio factor exposures\n    port_exposures = X_v.T @ w_v       # (K,)\n\n    # Factor risk\n    factor_var = port_exposures @ factor_cov @ port_exposures\n\n    # Specific risk\n    spec_var = w_v @ (sv_v * w_v)\n\n    # Total risk\n    total_var = factor_var + spec_var\n    total_vol = np.sqrt(total_var)\n\n    # Per-factor contribution\n    factor_mcr = factor_cov @ port_exposures  # marginal contribution\n    factor_contributions = port_exposures * factor_mcr\n\n    decomp = {\n        'Total Vol (ann %)': total_vol * 100,\n        'Factor Vol (ann %)': np.sqrt(factor_var) * 100,\n        'Specific Vol (ann %)': np.sqrt(spec_var) * 100,\n        'Factor Risk Share (%)': factor_var / total_var * 100 if total_var > 0 else 0.0,\n        'Specific Risk Share (%)': spec_var / total_var * 100 if total_var > 0 else 0.0,\n        'Portfolio Factor Exposures': pd.Series(port_exposures, index=factor_names),\n        'Factor Risk Contributions': pd.Series(\n            factor_contributions / total_var * 100 if total_var > 0\n            else factor_contributions * 0, index=factor_names),\n        'N Valid Stocks': n_valid,\n        'N Dropped (NaN)': n_dropped,\n    }\n\n    return decomp"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example portfolios — USE4 model with combined [industry | style] exposures\nT_last = T - 1\nspec_var = np.nanvar(residuals, axis=0, ddof=1)\n\n# Combined exposures at last period: Z = [D | X]\nZ_last = np.hstack([sector_dummies[T_last], exposures[T_last]])  # (N, I+K)\ncombined_names = sector_names + factor_names\n\n# Combined factor covariance (monthly)\ncombined_fret = np.hstack([results['industry_returns'],\n                           results['factor_returns']])  # (T, I+K)\ncombined_factor_cov_monthly = np.cov(combined_fret, rowvar=False)\n\n# Equal-weight portfolio\nw_eq = np.ones(N) / N\n\n# Cap-weight portfolio\nw_cap = market_cap / np.nansum(market_cap)\n\n# Random active portfolio (long-short, sum to 0)\nrng = np.random.RandomState(123)\nw_active = rng.randn(N)\nw_active = w_active - w_active.mean()\nw_active = w_active / np.abs(w_active).sum() * 2\n\nportfolios = {\n    'Equal-Weight': w_eq,\n    'Cap-Weight': w_cap,\n    'Long-Short Active': w_active\n}\n\nfor name, w in portfolios.items():\n    decomp = portfolio_risk_decomposition(\n        w, Z_last, combined_factor_cov_monthly, spec_var, combined_names)\n    print(f'\\n--- {name} Portfolio (USE4) ---')\n    print(f'  Stocks used: {decomp[\"N Valid Stocks\"]}  '\n          f'(dropped {decomp[\"N Dropped (NaN)\"]} with NaN)')\n    print(f'  Total Vol (monthly):   {decomp[\"Total Vol (ann %)\"]:.2f}%')\n    print(f'  Factor Vol (monthly):  {decomp[\"Factor Vol (ann %)\"]:.2f}%')\n    print(f'  Specific Vol (monthly):{decomp[\"Specific Vol (ann %)\"]:.2f}%')\n    print(f'  Factor Risk Share:     {decomp[\"Factor Risk Share (%)\"]:.1f}%')\n    print(f'  Specific Risk Share:   {decomp[\"Specific Risk Share (%)\"]:.1f}%')\n    print(f'  Industry Exposures:')\n    ind_exp = decomp['Portfolio Factor Exposures'][:I]\n    print(f'    {ind_exp.round(3).to_dict()}')\n    print(f'  Style Exposures:')\n    sty_exp = decomp['Portfolio Factor Exposures'][I:]\n    print(f'    {sty_exp.round(3).to_dict()}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Factor Exposure Turnover\n",
    "\n",
    "Factor exposure **turnover** measures how rapidly exposures change over time.\n",
    "High turnover implies a less stable factor definition. We measure turnover as\n",
    "the cross-sectional rank correlation of exposures between consecutive periods:\n",
    "\n",
    "$$\\text{Autocorr}_k = \\text{RankCorr}(X_{k,t}, X_{k,t-1})$$\n",
    "\n",
    "Values near 1.0 indicate stable, slowly-evolving exposures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_exposure_turnover(exposures, factor_names):\n",
    "    \"\"\"\n",
    "    Compute factor exposure turnover as rank autocorrelation.\n",
    "\n",
    "    Parameters:\n",
    "        exposures (np.array): Factor exposures, shape (T, N, K).\n",
    "        factor_names (list): Factor names.\n",
    "\n",
    "    Returns:\n",
    "        turnover_df (pd.DataFrame): Rank autocorrelation per factor per period.\n",
    "        turnover_summary (pd.DataFrame): Summary statistics.\n",
    "    \"\"\"\n",
    "    T, N, K = exposures.shape\n",
    "    autocorr = np.zeros((T - 1, K))\n",
    "\n",
    "    for t in range(1, T):\n",
    "        for k in range(K):\n",
    "            corr, _ = spearmanr(exposures[t, :, k], exposures[t - 1, :, k])\n",
    "            autocorr[t - 1, k] = corr\n",
    "\n",
    "    turnover_df = pd.DataFrame(autocorr, columns=factor_names)\n",
    "\n",
    "    turnover_summary = pd.DataFrame({\n",
    "        'Mean Rank Autocorr': turnover_df.mean(),\n",
    "        'Min Rank Autocorr': turnover_df.min(),\n",
    "        'Max Rank Autocorr': turnover_df.max()\n",
    "    })\n",
    "\n",
    "    return turnover_df, turnover_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_df, turnover_summary = factor_exposure_turnover(exposures, factor_names)\n",
    "\n",
    "print('Factor Exposure Turnover (Rank Autocorrelation):')\n",
    "print(turnover_summary.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "for k in range(K):\n",
    "    ax.plot(turnover_df[factor_names[k]], label=factor_names[k], linewidth=1, alpha=0.8)\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Rank Autocorrelation')\n",
    "ax.set_title('Factor Exposure Stability Over Time')\n",
    "ax.legend()\n",
    "ax.axhline(1.0, color='grey', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Variance Inflation Factor (VIF)\n",
    "\n",
    "The **VIF** measures multicollinearity among factor exposures. For factor $k$:\n",
    "\n",
    "$$\\text{VIF}_k = \\frac{1}{1 - R^2_k}$$\n",
    "\n",
    "where $R^2_k$ is the R-squared from regressing factor $k$'s exposures on all other factors.\n",
    "\n",
    "- VIF $\\leq$ 5: acceptable\n",
    "- VIF $>$ 10: severe multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vif(exposures, factor_names):\n",
    "    \"\"\"\n",
    "    Compute Variance Inflation Factors for factor exposures.\n",
    "\n",
    "    Parameters:\n",
    "        exposures (np.array): Factor exposures, shape (T, N, K).\n",
    "        factor_names (list): Factor names.\n",
    "\n",
    "    Returns:\n",
    "        vif_df (pd.DataFrame): VIF per factor, averaged and per-period.\n",
    "    \"\"\"\n",
    "    T, N, K = exposures.shape\n",
    "    vif_per_period = np.zeros((T, K))\n",
    "\n",
    "    for t in range(T):\n",
    "        X = exposures[t]  # (N, K)\n",
    "        for k in range(K):\n",
    "            y = X[:, k]\n",
    "            others = np.delete(X, k, axis=1)\n",
    "            # Add intercept\n",
    "            others_with_const = np.column_stack([np.ones(N), others])\n",
    "            # OLS: R-squared\n",
    "            beta = np.linalg.lstsq(others_with_const, y, rcond=None)[0]\n",
    "            y_hat = others_with_const @ beta\n",
    "            ss_res = np.sum((y - y_hat)**2)\n",
    "            ss_tot = np.sum((y - y.mean())**2)\n",
    "            r2 = 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "            vif_per_period[t, k] = 1.0 / (1.0 - r2) if r2 < 1.0 else np.inf\n",
    "\n",
    "    vif_mean = vif_per_period.mean(axis=0)\n",
    "    vif_max = vif_per_period.max(axis=0)\n",
    "\n",
    "    vif_df = pd.DataFrame({\n",
    "        'Mean VIF': vif_mean,\n",
    "        'Max VIF': vif_max,\n",
    "        'Status': ['OK' if v <= 5 else 'High' if v <= 10 else 'Severe'\n",
    "                    for v in vif_mean]\n",
    "    }, index=factor_names)\n",
    "\n",
    "    return vif_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df = compute_vif(exposures, factor_names)\n",
    "\n",
    "print('Variance Inflation Factors (VIF):')\n",
    "print(vif_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary Dashboard\n",
    "\n",
    "Consolidated view of all factor model diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summary_dashboard(fret_stats, ic_summary, quantile_summary,\n",
    "                             bias_df, vif_df, turnover_summary, r_squared):\n",
    "    \"\"\"\n",
    "    Build a consolidated summary of all factor model metrics.\n",
    "\n",
    "    Parameters:\n",
    "        fret_stats (pd.DataFrame): Factor return statistics.\n",
    "        ic_summary (pd.DataFrame): IC summary.\n",
    "        quantile_summary (pd.DataFrame): Quantile return summary.\n",
    "        bias_df (pd.DataFrame): Bias statistics.\n",
    "        vif_df (pd.DataFrame): VIF statistics.\n",
    "        turnover_summary (pd.DataFrame): Turnover summary.\n",
    "        r_squared (np.array): Cross-sectional R-squared.\n",
    "\n",
    "    Returns:\n",
    "        dashboard (pd.DataFrame): Consolidated metrics.\n",
    "    \"\"\"\n",
    "    dashboard = pd.DataFrame(index=fret_stats.index)\n",
    "\n",
    "    # Factor returns\n",
    "    dashboard['Ann. Return (%)'] = fret_stats['Ann. Mean (%)']\n",
    "    dashboard['Ann. Vol (%)'] = fret_stats['Ann. Vol (%)']\n",
    "    dashboard['Ret t-stat'] = fret_stats['t-stat']\n",
    "\n",
    "    # IC\n",
    "    dashboard['Mean IC'] = ic_summary['Mean IC']\n",
    "    dashboard['ICIR'] = ic_summary['ICIR']\n",
    "\n",
    "    # Quantile spread\n",
    "    dashboard['L/S Spread (%)'] = quantile_summary['L/S Spread'] * 100\n",
    "\n",
    "    # Bias\n",
    "    dashboard['Bias Stat'] = bias_df['Bias Statistic']\n",
    "\n",
    "    # VIF\n",
    "    dashboard['VIF'] = vif_df['Mean VIF']\n",
    "\n",
    "    # Turnover\n",
    "    dashboard['Exp. Autocorr'] = turnover_summary['Mean Rank Autocorr']\n",
    "\n",
    "    return dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard = build_summary_dashboard(\n",
    "    fret_stats, ic_summary, quantile_summary, bias_df, vif_df,\n",
    "    turnover_summary, r_squared)\n",
    "\n",
    "print('=' * 90)\n",
    "print('GRINOLD FACTOR MODEL METRICS — SUMMARY DASHBOARD')\n",
    "print('=' * 90)\n",
    "print(dashboard.round(3).to_string())\n",
    "print()\n",
    "print(f'Cross-Sectional R-squared: mean={r_squared.mean():.4f}, '\n",
    "      f'median={np.median(r_squared):.4f}')\n",
    "print('=' * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual summary: heatmap of key metrics (normalized for display)\n",
    "display_cols = ['Ret t-stat', 'ICIR', 'L/S Spread (%)', 'Bias Stat', 'VIF', 'Exp. Autocorr']\n",
    "display_df = dashboard[display_cols].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sns.heatmap(display_df, annot=True, fmt='.2f', cmap='RdYlGn', center=0, ax=ax)\n",
    "ax.set_title('Factor Model Metrics Summary')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Grinold, R. C. & Kahn, R. N. (1994). \"Multiple-Factor Models for Portfolio Risk.\" In *A Practitioner's Guide to Factor Models*, CFA Institute Research Foundation.\n",
    "- Grinold, R. C. & Kahn, R. N. (1999). *Active Portfolio Management*. McGraw-Hill.\n",
    "- Menchero, J., Orr, D. J., & Wang, J. (2011). \"The Barra US Equity Model (USE4).\" MSCI Methodology Notes.\n",
    "- Fama, E. F. & MacBeth, J. D. (1973). \"Risk, Return, and Equilibrium: Empirical Tests.\" *Journal of Political Economy*."
   ]
  }
 ]
}