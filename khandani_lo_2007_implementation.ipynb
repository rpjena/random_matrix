{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# What Happened to the Quants in August 2007?\n### Implementation of Khandani & Lo (2007) with Extension to February 2026\n\n> **Paper**: Khandani, A.E. and Lo, A.W. (2007). *\"What Happened to the Quants in August 2007?\"*\n> First Draft: September 20, 2007. MIT Laboratory for Financial Engineering.\n\n---\n**Notebook covers:**\n- Comprehensive paper notes and implementation plan\n- Core contrarian strategy (Equations 1â€“3)\n- Leverage choice (Equations 4â€“5) â€” Table 6 reproduction\n- Table 7: Leveraged daily returns for August 2007 (8:1 leverage)\n- Figure 4: Leveraged contrarian strategy vs market indexes (August 2007)\n- Extension to February 2026 with downloadable live data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“‹ Comprehensive Paper Notes\n\n### 1. Background & Context\nThe week of August 6, 2007 witnessed unprecedented losses for quantitative long/short equity hedge funds. Unlike the Bear Stearns and Sowood losses (credit-related), these losses hit **market-neutral** strategies that were supposed to be immune to market moves. The losses were confined almost exclusively to **quantitatively-driven** funds, hitting on Aug 7â€“8 when the S&P 500 actually *rose* both days.\n\n### 2. Key Hypotheses (The \"Unwind Hypothesis\")\nThe authors argue the losses were caused by a rapid cascade:\n1. **Initial trigger**: A large quant equity market-neutral portfolio was forcibly liquidated (likely margin calls from deteriorating credit/subprime positions).\n2. **Price impact**: The rapid selling of \"winner\" stocks and buying of \"loser\" stocks caused temporary but severe price dislocations.\n3. **Cascade**: Other similarly-constructed portfolios experienced losses â†’ triggered stop-losses â†’ more de-leveraging â†’ more price impact.\n4. **Reversal on Aug 10**: A large (+23.67% leveraged) reversal confirmed it was a liquidity trade, not a fundamental shift.\n\n### 3. The Contrarian Strategy\nFirst proposed by Lehmann (1990) and Lo & MacKinlay (1990). It is a **short-term mean-reversion** strategy:\n- **Universe**: All U.S. common stocks (CRSP codes 10 & 11), price $5â€“$2,000\n- **Signal**: Yesterday's losers are bought; yesterday's winners are sold\n- **Rebalancing**: Daily\n- **Structure**: Dollar-neutral (equal dollar long and short), market-neutral\n\nKey insight: This strategy **provides liquidity** to the market and profits from mean reversion and positive cross-autocorrelations across stocks.\n\n### 4. Alpha Decay (The Core Problem)\n| Year | Avg Daily Return | Commentary |\n|------|-----------------|------------|\n| 1995 | 1.38% | Enormous â€” frictionless theoretical max |\n| 1998 | 0.57% | Paper's baseline year |\n| 2003 | 0.21% | Post-dot-com decline |\n| 2006 | 0.15% | Near-floor profitability |\n| 2007 | 0.13% | Almost 10Ã— decline from 1995 |\n\n**Why the decline?** Increased competition, decimalization (2001), lower volatility, better technology, more capital chasing same signals.\n\n### 5. The Leverage Trap (Section 6)\nAs alpha decayed, managers **increased leverage** to maintain target returns. Using 1998 as baseline:\n- **Equation 4**: Set E[L_pt] = (Î¸/2)Â·E[R_pt] = E[R_p,1998]\n- **Equation 5**: Î¸* = 2Â·E[R_p,1998] / E[R_pt]\n\nBy 2006, the required leverage was ~**8:1** (vs 2:1 Reg-T). By 2007 (ex-ante), ~**9:1**.\nThis is why a 12-Ïƒ unleveraged event became a ~48-Ïƒ leveraged disaster.\n\n### 6. August 2007 vs August 1998\nA crucial comparison: in August 1998 (LTCM crisis, Russia default), the contrarian strategy showed **no unusual behavior** â€” it was profitable and calm every day. In August 2007, it lost 12 daily standard deviations in 3 days.\n\nThis signals dramatically increased **financial market integration** and systemic risk by 2007. The LTCM crisis was contained in fixed-income; the 2007 crisis crossed all asset classes.\n\n### 7. Crowded Trade (Figure 3 / TASS Data)\n- Long/Short Equity Hedge + Equity Market Neutral AUM grew from ~$20B (1995) to >$160B (2007)\n- Average assets per fund grew from $62MM (1994) to $229MM (2007) â€” exponentially\n- More capital â†’ more competition â†’ lower alpha â†’ higher required leverage â†’ higher systemic risk\n\n### 8. The Leveraged Returns (Table 7 / Figure 4)\nUsing Î¸ = 8 (the 2006 estimated leverage level), multiplied by Î¸/2 = 4:\n| Date | Unleveraged | Leveraged (8:1) | S&P 500 |\n|------|-------------|-----------------|---------|\n| Aug 7 | -1.16% | **-4.64%** | +0.62% |\n| Aug 8 | -2.83% | **-11.33%** | +1.44% |\n| Aug 9 | -2.86% | **-11.43%** | -2.95% |\n| Aug 10 | +5.92% | **+23.67%** | +0.04% |\n\nFunds that capitulated on Aug 8â€“9 missed the reversal on Aug 10.\n\n### 9. Systemic Risk Implications\n- Greater hedge-fund connectivity (higher pairwise correlations across CS/Tremont indexes)\n- Increasing illiquidity of supposedly liquid long/short equity funds (positive autocorrelation in monthly returns)\n- 130/30 strategies adding $75B+ of long/short activity with similar factor exposures\n- Authors conclude the \"Doomsday Clock\" for systemic risk stands at ~11:51pm\n\n### 10. Implementation Notes for This Notebook\n- **1995â€“2007**: Uses exact data from the paper (Tables 2, 3, 6, 7)\n- **2008â€“2026**: Uses yfinance data (S&P 500 proxy universe â€” ~500 stocks vs ~4,000 in CRSP)\n- **Key caveat**: S&P 500 is survivorship-biased and larger-cap than CRSP; small-cap mean reversion is understated\n- **Leverage baseline**: Maintained at 1998 level (E[Rp,1998] = 0.57%/day) throughout"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as mticker\nfrom matplotlib.gridspec import GridSpec\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# â”€â”€ Plotting style â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nplt.rcParams.update({\n    'figure.dpi': 120,\n    'font.size': 11,\n    'axes.titlesize': 13,\n    'axes.labelsize': 11,\n    'lines.linewidth': 1.8,\n    'grid.alpha': 0.3,\n    'axes.spines.top': False,\n    'axes.spines.right': False,\n})\n\nprint(\"âœ… Imports complete.\")\nprint(\"Python version:\", __import__('sys').version.split()[0])\nprint(\"pandas:\", pd.__version__, \"| numpy:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part I: Core Strategy Functions (Equations 1â€“5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  EQUATION 1 â€” Contrarian Portfolio Weights\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Ï‰_{it} = -(1/N) Â· (R_{i,t-1} âˆ’ R_{m,t-1})\n# R_{m,t-1} = (1/N) Â· Î£ R_{i,t-1}   [equal-weighted market return]\n\ndef contrarian_weights(prev_returns: pd.Series) -> pd.Series:\n    \"\"\"\n    Compute Equation (1): contrarian portfolio weights.\n    prev_returns : Series of stock returns at t-1 (already cleaned/filtered).\n    Returns      : Series of weights (dollar-neutral by construction).\n    \"\"\"\n    prev_returns = prev_returns.dropna()\n    N = len(prev_returns)\n    if N == 0:\n        return pd.Series(dtype=float)\n    Rm = prev_returns.mean()          # equal-weighted market return\n    omega = -(1.0 / N) * (prev_returns - Rm)\n    return omega\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  EQUATION 2 â€” Gross Investment & Unleveraged Return\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# I_t = (1/2) Â· Î£ |Ï‰_{it}|\n# R_pt = Î£ Ï‰_{it}Â·R_{it} / I_t\n\ndef unleveraged_return(omega: pd.Series, curr_returns: pd.Series) -> float:\n    \"\"\"\n    Compute Equation (2): unleveraged portfolio return for one day.\n    omega        : weights from contrarian_weights()\n    curr_returns : current-day returns for the same stocks\n    Returns      : scalar unleveraged return R_pt\n    \"\"\"\n    # Align on common tickers, drop any NaN\n    common = omega.index.intersection(curr_returns.dropna().index)\n    omega_c   = omega.loc[common]\n    returns_c = curr_returns.loc[common]\n\n    It = 0.5 * omega_c.abs().sum()    # gross investment\n    if It == 0:\n        return np.nan\n    profit = (omega_c * returns_c).sum()\n    return profit / It\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  EQUATION 3 â€” Leveraged Return\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# L_pt(Î¸) = (Î¸/2) Â· R_pt\n\ndef leveraged_return(Rpt: float, theta: float) -> float:\n    \"\"\"Compute Equation (3): leveraged return given leverage ratio Î¸:1.\"\"\"\n    return (theta / 2.0) * Rpt\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  EQUATIONS 4 & 5 â€” Optimal Leverage Choice\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Target: E[L_pt] = (Î¸*/2)Â·E[R_pt] = E[R_p,1998]\n# Solution: Î¸* = 2Â·E[R_p,1998] / E[R_pt]\n\nE_Rp_1998 = 0.57 / 100.0   # 0.57% per day â†’ decimal\n\ndef optimal_leverage(E_Rpt: float,\n                     E_Rp_baseline: float = E_Rp_1998) -> tuple:\n    \"\"\"\n    Compute Equations (4)+(5): optimal leverage and return multiplier.\n    E_Rpt           : average daily return in year t (decimal)\n    E_Rp_baseline   : baseline expected daily return (1998 level)\n    Returns: (return_multiplier, leverage_ratio Î¸*)\n    \"\"\"\n    if E_Rpt <= 0:\n        return np.nan, np.nan\n    multiplier = E_Rp_baseline / E_Rpt          # return multiplier\n    theta_star = 2.0 * E_Rp_baseline / E_Rpt   # Eq. (5)\n    return multiplier, theta_star\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  VECTORISED STRATEGY â€” Apply to a price DataFrame\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\ndef compute_contrarian_strategy(\n        prices_df: pd.DataFrame,\n        min_price: float = 5.0,\n        max_price: float = 2000.0) -> pd.Series:\n    \"\"\"\n    Apply contrarian strategy to a price DataFrame in a vectorised way.\n\n    prices_df : DataFrame, index=dates, columns=ticker symbols, values=close prices\n    Returns   : Series of UNLEVERAGED daily returns (R_pt)\n\n    Algorithm:\n      1. Compute daily pct returns.\n      2. For each day t, use t-1 prices to create valid-stock mask ($5-$2,000).\n      3. Compute equal-weighted market return of valid stocks at t-1.\n      4. Weights Ï‰_{it} = -(R_{i,t-1} âˆ’ Rm_{t-1}) / N   [Eq.1]\n      5. I_t = 0.5Â·Î£|Ï‰_{it}|                              [Eq.2 numerator]\n      6. R_pt = Î£(Ï‰_{it}Â·R_{it}) / I_t                    [Eq.2 return]\n    \"\"\"\n    prices = prices_df.copy()\n\n    # Step 1: returns\n    rets = prices.pct_change()\n\n    # Step 2: price validity mask (based on PREVIOUS day's closing prices)\n    prev_prices = prices.shift(1)\n    valid = (prev_prices >= min_price) & (prev_prices <= max_price)\n\n    # Mask returns to valid stocks only\n    r_prev = rets.shift(1).where(valid)    # R_{i,t-1} (previous day, valid stocks)\n    r_curr = rets.where(valid)             # R_{i,t}   (current day, same mask)\n\n    # Step 3: equal-weighted market return at t-1\n    N      = valid.sum(axis=1)             # number of valid stocks per day\n    Rm     = r_prev.mean(axis=1)          # mean of r_prev (skipna by default)\n\n    # Step 4: weights  Ï‰_{it} = -(R_{i,t-1} âˆ’ Rm_{t-1}) / N\n    excess  = r_prev.subtract(Rm, axis=0) # shape (T, N)\n    omega   = -excess.div(N, axis=0)\n\n    # Step 5: gross investment\n    It = 0.5 * omega.abs().sum(axis=1)\n\n    # Step 6: profit and return\n    profit = (omega * r_curr).sum(axis=1)\n    Rpt    = profit / It\n    Rpt    = Rpt.replace([np.inf, -np.inf], np.nan)\n\n    return Rpt\n\n\nprint(\"âœ… Core functions defined (Equations 1-5).\")\nprint(f\"   Baseline 1998 average daily return: {E_Rp_1998:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part II: Reproducing Paper Results (1995â€“2007)\n\n### Table 2 â€” Annual Statistics of Contrarian Strategy (All Stocks)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Table 2: Year-by-Year Average Daily Returns (from paper, \"All\" column) â”€â”€\npaper_years = list(range(1995, 2008))\n\n# Average daily return (%) â€” \"All\" column, Table 2\npaper_avg_daily_ret = {\n    1995: 1.38, 1996: 1.17, 1997: 0.88, 1998: 0.57, 1999: 0.44,\n    2000: 0.44, 2001: 0.31, 2002: 0.45, 2003: 0.21, 2004: 0.37,\n    2005: 0.26, 2006: 0.15, 2007: 0.13,\n}\n# Standard deviation of daily returns (%) â€” \"All\" column, Table 2\npaper_std_daily_ret = {\n    1995: 0.40, 1996: 0.48, 1997: 0.68, 1998: 0.84, 1999: 1.02,\n    2000: 1.68, 2001: 1.43, 2002: 0.98, 2003: 0.54, 2004: 0.53,\n    2005: 0.46, 2006: 0.52, 2007: 0.72,\n}\n# Annualised Sharpe (âˆš250 Ã— mean/std) â€” \"All\" column, Table 2\npaper_sharpe = {\n    1995: 53.87, 1996: 38.26, 1997: 20.46, 1998: 10.62, 1999: 6.81,\n    2000:  4.17, 2001:  3.46, 2002:  7.25, 2003:  5.96, 2004: 11.07,\n    2005:  8.85, 2006:  4.47, 2007:  2.79,\n}\n\ndf_table2 = pd.DataFrame({\n    'Year':           paper_years,\n    'Avg_Daily_Ret%': [paper_avg_daily_ret[y] for y in paper_years],\n    'Std_Daily_Ret%': [paper_std_daily_ret[y] for y in paper_years],\n    'Ann_Sharpe':     [paper_sharpe[y]         for y in paper_years],\n}).set_index('Year')\n\nprint(\"Table 2 â€” Annual Statistics (Paper Data, 'All Stocks' column)\")\nprint(\"=\"*62)\nprint(df_table2.to_string())\n\n# Quick verification of Sharpe formula\ndf_table2['Sharpe_Computed'] = (\n    df_table2['Avg_Daily_Ret%'] / df_table2['Std_Daily_Ret%'] * np.sqrt(250)\n)\nprint(\"\\nSharpe ratio cross-check (computed vs paper) â€” max discrepancy:\",\n      (df_table2['Sharpe_Computed'] - df_table2['Ann_Sharpe']).abs().max().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Figure 1: Secular Decline in Average Daily Returns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfig, ax = plt.subplots(figsize=(11, 5))\nax.bar(df_table2.index, df_table2['Avg_Daily_Ret%'],\n       color='steelblue', edgecolor='white', alpha=0.85, label='Avg Daily Return (All)')\nax.axhline(0.57, color='red', linestyle='--', linewidth=1.2, label='1998 baseline (0.57%/day)')\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Average Daily Return (%)\")\nax.set_title(\"Figure 1: Secular Decline of Contrarian Strategy Returns, 1995-2007\\n(Reproducing Khandani & Lo 2007, Figure 1 concept)\")\nax.legend()\nax.set_xticks(paper_years)\nax.tick_params(axis='x', rotation=45)\nax.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f%%'))\nplt.tight_layout()\nplt.savefig('fig1_alpha_decay.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"âœ… Alpha decay clearly visible: 1.38%/day (1995) â†’ 0.13%/day (2007)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Table 6 â€” Leverage Required to Match 1998 Returns (Equations 4 & 5)\n\n**Equation 4** (target return condition):\n$$E[L_{pt}] \\;=\\; \\frac{\\theta^*}{2} \\cdot E[R_{pt}] \\;=\\; E[R_{p,1998}]$$\n\n**Equation 5** (optimal leverage):\n$$\\theta^* \\;=\\; \\frac{2 \\cdot E[R_{p,1998}]}{E[R_{pt}]}, \\quad t = 1999, 2000, \\ldots$$\n\nWe use 1998 as baseline because it represents the \"expectations level\" that investors had before alpha decay became severe. The factor of 2 arises from the definition of leverage (sum of absolute long + short positions Ã· capital)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Table 6: Leverage Calculation (Equations 4 & 5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nE_Rp_1998_pct = 0.57   # percent\n\n# Paper Table 6 (exact) â€” includes these years\ntable6_years = [1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007]\ntable6_returns = {\n    1998: 0.57, 1999: 0.44, 2000: 0.44, 2001: 0.31,\n    2002: 0.45, 2003: 0.21, 2004: 0.37, 2005: 0.26,\n    2006: 0.15, 2007: 0.13,\n}\n\ndf_table6 = pd.DataFrame({\n    'Year':             table6_years,\n    'Avg_Daily_Ret%':   [table6_returns[y] for y in table6_years],\n}).set_index('Year')\n\n# Equations 4 & 5\ndf_table6['Return_Multiplier'] = E_Rp_1998_pct / df_table6['Avg_Daily_Ret%']   # 1/scalar\ndf_table6['Leverage_Ratio_Î¸*'] = 2.0 * E_Rp_1998_pct / df_table6['Avg_Daily_Ret%']  # Eq.5\n\nprint(\"Table 6 â€” Leverage Required to Match 1998 Expected Return\")\nprint(\"(E[R_p,1998] = 0.57%/day as baseline)\")\nprint(\"=\"*65)\nheader = f\"{'Year':>6} {'Avg Daily Return':>18} {'Return Multiplier':>18} {'Leverage Ratio Î¸*':>18}\"\nprint(header)\nprint(\"-\"*65)\nfor yr, row in df_table6.iterrows():\n    print(f\"{yr:>6} {row['Avg_Daily_Ret%']:>17.2f}%   {row['Return_Multiplier']:>17.2f}    {row['Leverage_Ratio_Î¸*']:>17.2f}:1\")\n\nprint(\"\\nğŸ“Œ Paper's exact Table 6 values (cross-check):\")\nprint(\"   1999: multiplier=1.28, leverage=2.57\")\nprint(\"   2006: multiplier=3.88, leverage=7.76\")\nprint(\"   2007: multiplier=4.48, leverage=8.96\")\nprint(f\"\\n   Our 2006: {df_table6.loc[2006,'Return_Multiplier']:.2f}, {df_table6.loc[2006,'Leverage_Ratio_Î¸*']:.2f}:1  âœ“\")\nprint(f\"   Our 2007: {df_table6.loc[2007,'Return_Multiplier']:.2f}, {df_table6.loc[2007,'Leverage_Ratio_Î¸*']:.2f}:1  âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Leverage Plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfig, ax1 = plt.subplots(figsize=(11, 5))\n\ncolor1, color2 = 'steelblue', 'tomato'\nax1.bar(df_table6.index, df_table6['Avg_Daily_Ret%'],\n        color=color1, alpha=0.7, label='Avg Daily Return % (left)')\nax1.set_xlabel(\"Year\")\nax1.set_ylabel(\"Average Daily Return (%)\", color=color1)\nax1.tick_params(axis='y', labelcolor=color1)\nax1.set_ylim(0, 0.65)\n\nax2 = ax1.twinx()\nax2.plot(df_table6.index, df_table6['Leverage_Ratio_Î¸*'],\n         color=color2, marker='o', linewidth=2, label='Required Leverage Î¸* (right)')\nax2.set_ylabel(\"Required Leverage Ratio Î¸* (Eq. 5)\", color=color2)\nax2.tick_params(axis='y', labelcolor=color2)\nax2.axhline(8, color=color2, linestyle=':', alpha=0.4, linewidth=1)\nax2.text(2006.5, 8.2, 'Î¸=8 used for Table 7', fontsize=9, color=color2)\n\nax1.set_title(\"Equations 4 & 5: As Alpha Decays, Required Leverage Explodes\\n(Khandani & Lo 2007, Table 6)\")\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\nax1.set_xticks(table6_years)\nplt.tight_layout()\nplt.savefig('fig_leverage_buildup.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part III: Table 7 & Figure 4 â€” August 2007 Leveraged Returns\n\nTable 7 = Table 3 Ã— (Î¸/2) = Table 3 Ã— **4**  (since Î¸ = 8:1 leverage)\n\nWe reproduce both the \"All Stocks\" column and all 10 market-cap decile columns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Table 3 Data: Unleveraged Daily Returns â€” Full Decile Matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Date, Smallest, D2, D3, D4, D5, D6, D7, D8, D9, Largest, All\ntable3_raw = [\n    ('7/30/2007', -0.07,  0.02,  1.96, -0.36,  0.07,  0.23,  0.26,  0.38,  0.51,  0.18,  0.44),\n    ('7/31/2007',  0.19,  1.10,  0.28,  0.55, -0.63,  0.02, -0.80,  0.49, -0.31,  0.06,  0.36),\n    ('8/1/2007',   1.53,  0.45, -1.39,  0.35,  0.95, -0.88, -0.71, -0.63, -2.02, -0.22,  0.11),\n    ('8/2/2007',   0.88, -0.76, -0.12, -0.67, -0.94, -2.70,  2.16,  1.53, -0.74, -0.19, -0.30),\n    ('8/3/2007',  -0.95, -0.62, -0.78,  0.06,  0.88,  0.01, -0.62, -1.09, -0.57, -0.68, -0.02),\n    ('8/6/2007',  -0.83, -1.77, -0.39, -1.03,  1.37, -1.37, -1.19, -0.72,  0.27,  0.77,  0.50),\n    ('8/7/2007',   0.75,  0.26, -1.64, -2.91, -1.50, -0.70,  0.36, -1.02, -1.72, -0.67, -1.16),\n    ('8/8/2007',   0.88, -1.33, -2.59, -3.65, -4.27, -2.16, -2.23, -3.46, -1.26, -1.48, -2.83),\n    ('8/9/2007',   0.91, -1.86, -3.87, -2.77, -3.18, -3.95, -3.27, -4.33, -2.58, -1.31, -2.86),\n    ('8/10/2007', -0.33,  3.65,  6.08,  7.90,  8.77,  7.67,  7.52,  6.70,  4.68,  2.39,  5.92),\n    ('8/13/2007',  1.36, -0.31, -0.63, -1.07, -1.55, -0.22, -1.29, -2.01, -2.14, -1.25, -0.76),\n    ('8/14/2007',  1.16,  0.91, -0.26,  0.34,  0.56, -0.28,  0.69, -0.29,  0.16,  0.17,  0.08),\n    ('8/15/2007',  0.88,  1.19, -0.61, -0.58, -0.17, -0.97, -0.24, -1.34, -0.57, -1.18, -0.38),\n    ('8/16/2007', -1.26, -0.54,  0.15, -0.59, -0.60, -0.99, -1.73, -1.27,  0.27, -1.83, -0.81),\n    ('8/17/2007',  3.57,  2.49,  0.10,  1.26,  1.33, -0.52,  0.12, -0.39,  0.31,  0.11,  0.38),\n    ('8/20/2007',  3.75,  1.75,  0.35,  1.35,  0.51,  0.44,  1.22,  0.56,  0.39,  1.17,  1.14),\n    ('8/21/2007',  1.24,  0.11,  0.01, -0.45,  0.02, -0.63, -0.08, -0.05,  0.19,  0.11,  0.06),\n    ('8/22/2007', -0.85, -0.31, -0.52, -0.51, -0.17, -0.83, -0.18, -0.56,  0.39,  0.09, -0.38),\n    ('8/23/2007', -0.03,  0.70,  0.70, -0.16,  0.38,  1.04,  0.26, -0.33,  0.32,  0.31,  0.33),\n    ('8/24/2007',  0.62, -0.28, -0.07,  0.23,  0.92, -0.06, -0.07,  0.09, -0.35,  0.61,  0.43),\n    ('8/27/2007',  1.10,  0.70,  0.11,  0.20,  1.25, -0.16,  0.39,  0.71,  0.71,  0.03,  0.75),\n    ('8/28/2007',  0.41,  0.32,  0.08, -0.61, -0.64, -0.50, -0.33, -0.44, -0.47,  0.25, -0.76),\n    ('8/29/2007',  1.45,  0.08,  1.27,  2.08,  1.94, -0.53,  1.42,  1.60,  0.91,  0.98,  1.76),\n    ('8/30/2007',  1.07,  0.04,  0.62,  0.40,  0.89,  0.10, -0.03, -0.04,  0.12, -0.05,  0.50),\n    ('8/31/2007',  1.69,  0.97,  0.95, -0.55,  0.05,  0.52, -0.08, -0.67,  0.01,  0.14,  0.36),\n]\n\ncols = ['Date','Smallest','D2','D3','D4','D5','D6','D7','D8','D9','Largest','All']\ndf_table3 = pd.DataFrame(table3_raw, columns=cols)\ndf_table3['Date'] = pd.to_datetime(df_table3['Date'])\ndf_table3 = df_table3.set_index('Date')\n\n# â”€â”€ Compute Table 7 (Eq. 3 with Î¸ = 8) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTHETA = 8\nMULTIPLIER = THETA / 2   # = 4\n\ndf_table7 = df_table3 * MULTIPLIER\n\nprint(f\"Table 7 = Table 3 Ã— {MULTIPLIER:.0f}  (Î¸={THETA}:1 leverage, multiplier = Î¸/2 = {MULTIPLIER:.0f})\")\nprint(\"=\"*110)\nprint(df_table7.round(2).to_string())\nprint()\n# Verify key cells against paper\nchecks = {\n    '8/7/2007 All':  (df_table7.loc['2007-08-07','All'],  -4.64),\n    '8/8/2007 All':  (df_table7.loc['2007-08-08','All'], -11.33),\n    '8/9/2007 All':  (df_table7.loc['2007-08-09','All'], -11.43),\n    '8/10/2007 All': (df_table7.loc['2007-08-10','All'], +23.67),\n}\nprint(\"\\nğŸ“Œ Cross-checks vs paper (Table 7):\")\nfor label, (computed, expected) in checks.items():\n    flag = 'âœ“' if abs(computed - expected) < 0.05 else 'âœ—'\n    print(f\"   {label}: computed={computed:.2f}%  expected={expected:.2f}%  {flag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Figure 4 â€” Leveraged Contrarian Strategy vs Market Indexes (August 2007)\n\nFigure 4 in the paper plots the leveraged daily returns of the contrarian strategy (8:1 leverage)\nalongside S&P 500, S&P Small Cap 600, Lehman US Gov Aggregate, GSCI, and Trade-Weighted USD."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Market Index Data for August 2007 (from paper Table 4) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Date, S&P500, S&P600, MSCI_EM, MSCI_WorldExUS, Lehman_Agg, Lehman_HY, GSCI, USD, VIX_chg\ntable4_raw = [\n    ('7/30/2007',  1.03,  0.94,  0.87,  0.14, -0.04,  0.18,  0.11, -0.12, -3.30),\n    ('7/31/2007', -1.26, -0.88,  1.67,  1.36,  0.17,  0.61,  1.18, -0.10,  2.65),\n    ('8/1/2007',   0.73,  0.19, -3.42, -1.70,  0.04, -0.15, -1.34,  0.13,  0.15),\n    ('8/2/2007',   0.46,  0.98,  0.61,  0.62,  0.04,  0.53,  0.00, -0.20, -2.45),\n    ('8/3/2007',  -2.65, -3.48, -0.05, -0.37,  0.29,  0.08, -1.10, -0.66,  3.94),\n    ('8/6/2007',   2.42,  1.35, -1.99, -0.57, -0.14, -0.29, -2.76,  0.10, -2.56),\n    ('8/7/2007',   0.62,  0.71,  0.45,  0.56, -0.04,  0.38,  0.34,  0.28, -1.04),\n    ('8/8/2007',   1.44,  1.52,  2.83,  1.88, -0.48,  0.84, -0.20, -0.17, -0.11),\n    ('8/9/2007',  -2.95, -1.38, -1.28, -1.52,  0.31, -0.07, -0.37,  0.54,  5.03),\n    ('8/10/2007',  0.04,  1.01, -3.30, -2.85,  0.07, -0.29, -0.03, -0.12,  1.82),\n    ('8/13/2007', -0.03, -0.84,  1.01,  1.08,  0.04,  0.34,  0.27,  0.46, -1.73),\n    ('8/14/2007', -1.81, -1.87, -1.42, -1.10,  0.23, -0.10,  0.35,  0.54,  1.11),\n    ('8/15/2007', -1.36, -1.45, -2.39, -1.52,  0.15, -0.56,  0.80,  0.41,  2.99),\n    ('8/16/2007',  0.33,  1.70, -5.63, -2.91,  0.58, -0.59, -3.01, -0.11,  0.16),\n    ('8/17/2007',  2.46,  2.30,  0.12,  0.96, -0.28,  0.24,  1.49, -0.37, -0.84),\n    ('8/20/2007', -0.03,  0.30,  3.78,  1.23,  0.23,  0.24, -1.65, -0.03, -3.66),\n    ('8/21/2007',  0.11,  0.21, -0.18,  0.61,  0.24,  0.19, -1.14,  0.11, -1.08),\n    ('8/22/2007',  1.18,  1.19,  2.58,  1.27, -0.16,  0.37,  0.04, -0.30, -2.36),\n    ('8/23/2007', -0.11, -1.16,  1.76,  1.16, -0.01,  0.22,  0.96, -0.13, -0.27),\n    ('8/24/2007',  1.16,  1.44,  0.44,  0.51, -0.10,  0.04,  1.10, -0.59, -1.90),\n    ('8/27/2007', -0.85, -1.07,  1.90,  0.29,  0.23,  0.17,  0.28,  0.09,  2.00),\n    ('8/28/2007', -2.34, -2.70, -0.85, -1.26,  0.34, -0.07, -0.17,  0.02,  3.58),\n    ('8/29/2007',  2.22,  2.28, -0.23,  0.04, -0.09, -0.06,  1.40, -0.07, -2.49),\n    ('8/30/2007', -0.41, -0.38,  1.31,  0.80,  0.29,  0.06,  0.15,  0.12,  1.25),\n    ('8/31/2007',  1.12,  1.28,  2.39,  1.58, -0.16,  0.01,  0.48,  0.00, -1.68),\n]\n\nidx_cols = ['Date','SP500','SP600','MSCI_EM','MSCI_WorldExUS',\n            'Lehman_Agg','Lehman_HY','GSCI','USD','VIX_change']\ndf_table4 = pd.DataFrame(table4_raw, columns=idx_cols)\ndf_table4['Date'] = pd.to_datetime(df_table4['Date'])\ndf_table4 = df_table4.set_index('Date')\n\n# â”€â”€ Figure 4: Leverage Contrarian Strategy vs Indexes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Paper shows: Leveraged Contrarian, S&P 500, S&P 600 Small Cap, Lehman Corp-Gov, GSCI, USD\n# Filter to August only (paper's Figure 4 scope)\naug_dates = df_table7.index[df_table7.index >= '2007-08-01']\n\nfig, ax = plt.subplots(figsize=(14, 7))\n\n# Leveraged contrarian (thick, highlighted)\nax.plot(aug_dates, df_table7.loc[aug_dates, 'All'],\n        color='navy', linewidth=2.8, marker='o', markersize=5,\n        label='Leveraged Contrarian Strategy (8:1)', zorder=5)\n\n# Market indexes\nstyles = [\n    ('SP500',      'S&P 500',          'tomato',   '-',  1.6),\n    ('SP600',      'S&P 600 Small Cap','darkorange','-',  1.4),\n    ('Lehman_Agg', 'Lehman Corp-Gov',  'green',    '--', 1.4),\n    ('GSCI',       'GSCI Commodity',   'purple',   '-.',  1.3),\n    ('USD',        'Trade-Wtd USD',    'gray',     ':',   1.3),\n]\nfor col, label, color, ls, lw in styles:\n    ax.plot(df_table4.loc[aug_dates].index, df_table4.loc[aug_dates, col],\n            color=color, linestyle=ls, linewidth=lw, label=label, alpha=0.85)\n\n# Annotate key events\nax.annotate('Aug 7: âˆ’4.64%\\n(S&P +0.62%)', xy=(pd.Timestamp('2007-08-07'), -4.64),\n            xytext=(pd.Timestamp('2007-08-07'), -8.5),\n            arrowprops=dict(arrowstyle='->', color='navy', lw=1.5),\n            fontsize=9, color='navy', ha='center')\nax.annotate('Aug 8: âˆ’11.33%', xy=(pd.Timestamp('2007-08-08'), -11.33),\n            xytext=(pd.Timestamp('2007-08-09'), -14.5),\n            arrowprops=dict(arrowstyle='->', color='navy', lw=1.5),\n            fontsize=9, color='navy', ha='center')\nax.annotate('Aug 10: +23.67%\\n(Reversal)', xy=(pd.Timestamp('2007-08-10'), 23.67),\n            xytext=(pd.Timestamp('2007-08-14'), 22),\n            arrowprops=dict(arrowstyle='->', color='darkgreen', lw=1.5),\n            fontsize=9, color='darkgreen', ha='left')\n\nax.axhline(0, color='black', linewidth=0.8, alpha=0.5)\nax.axvspan(pd.Timestamp('2007-08-07'), pd.Timestamp('2007-08-09'),\n           color='red', alpha=0.06, label='Crisis Days (Aug 7-9)')\nax.set_xlabel(\"Date (August 2007)\")\nax.set_ylabel(\"Daily Return (%)\")\nax.set_title(\"Figure 4: Leveraged Daily Returns â€” Contrarian Strategy vs Market Indexes, August 2007\\n\"\n             \"(8:1 Leverage, Multiplier=4 â€” Khandani & Lo 2007, Figure 4 Reproduction)\")\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\nax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=0))\nplt.xticks(rotation=30)\nax.legend(loc='upper right', fontsize=9, framealpha=0.9)\nax.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.1f%%'))\nplt.tight_layout()\nplt.savefig('fig4_leveraged_returns_aug2007.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cumulative Leveraged Returns During August 2007 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Arithmetic compounding (as used in the paper)\ncrisis_window = df_table7.loc['2007-08-06':'2007-08-10', 'All']\nprint(\"=== Cumulative Losses/Gains: Aug 6-10, 2007 (8:1 Leverage) ===\")\ncumulative = 0.0\nfor date, ret in crisis_window.items():\n    cumulative += ret\n    print(f\"  {date.strftime('%b %d')}: {ret:+.2f}%  |  Cumulative: {cumulative:+.2f}%\")\n\nprint()\n# Three-day loss Aug 7-9\nloss_3d = df_table7.loc['2007-08-07':'2007-08-09', 'All'].sum()\nprint(f\"ğŸ“‰ Three-day cumulative loss (Aug 7-9):  {loss_3d:.2f}%  (â‰ˆ âˆ’25% of capital)\")\nprint(f\"ğŸ“ˆ Aug 10 reversal:                     +{df_table7.loc['2007-08-10','All']:.2f}%\")\nprint(f\"ğŸ“Š Full week (Aug 6-10):                {df_table7.loc['2007-08-06':'2007-08-10','All'].sum():.2f}% (arithmetic)\")\nprint()\nprint(\"=== Standard Deviation Context ===\")\nsigma_2006 = 0.52  # percent (paper's 2006 std dev, used as benchmark)\nprint(f\"2006 daily std deviation: {sigma_2006}%\")\nprint(f\"Aug 7 unleveraged loss:  {df_table3.loc['2007-08-07','All']:.2f}% â†’ \"\n      f\"{abs(df_table3.loc['2007-08-07','All'])/sigma_2006:.1f}Ïƒ\")\nthree_day_unlev = df_table3.loc['2007-08-07':'2007-08-09','All'].sum()\nprint(f\"Aug 7-9 cumulative:      {three_day_unlev:.2f}% â†’ {abs(three_day_unlev)/sigma_2006:.1f}Ïƒ  (â‰ˆ12Ïƒ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part IV: Extension to February 2026\n\nThis section downloads live stock data from Yahoo Finance (via `yfinance`) and applies the contrarian strategy to extend the analysis from 2008 to February 2026.\n\n**Key methodological differences from the paper:**\n- **Universe**: S&P 500 current constituents (~500 stocks) vs CRSP all common stocks (~3,600â€“5,300)\n- **Survivorship bias**: Current S&P 500 list excludes delisted companies\n- **Small-cap exposure**: Much reduced vs CRSP universe (strategy performance is understated, especially for small-cap deciles)\n- **Price filter**: Same as paper ($5â€“$2,000)\n- **Leverage baseline**: Same as paper (E[Rp,1998] = 0.57%/day)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#  DATA DOWNLOAD â€” Yahoo Finance via yfinance\n#  Install if needed:  pip install yfinance\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\ntry:\n    import yfinance as yf\n    _yf_available = True\n    print(\"âœ… yfinance available\")\nexcept ImportError:\n    _yf_available = False\n    print(\"âš ï¸  yfinance not installed. Run:  pip install yfinance\")\n    print(\"   Then re-run this cell to download live data.\")\n    print(\"   The notebook will use estimated/paper data for the extension section.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if _yf_available:\n    # â”€â”€ Get S&P 500 tickers from Wikipedia â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    try:\n        sp500_df = pd.read_html(\n            'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n        )[0]\n        tickers = sp500_df['Symbol'].str.replace('.', '-', regex=False).tolist()\n        print(f\"âœ… Downloaded {len(tickers)} S&P 500 tickers from Wikipedia\")\n    except Exception as e:\n        print(f\"âš ï¸  Wikipedia unavailable ({e}). Using representative 200-ticker sample.\")\n        # Representative cross-section of large/mid-cap US stocks\n        tickers = [\n            'AAPL','MSFT','AMZN','GOOGL','META','NVDA','TSLA','JPM','JNJ','V',\n            'UNH','HD','PG','MA','BAC','XOM','ABBV','MRK','CVX','KO',\n            'PEP','LLY','AVGO','COST','TMO','MCD','ABT','ACN','NEE','CSCO',\n            'DHR','WMT','VZ','TXN','ADBE','CRM','PM','CMCSA','NKE','QCOM',\n            'RTX','HON','IBM','UPS','BMY','AMGN','CAT','MS','GS','BA',\n            'SBUX','INTU','AMAT','ISRG','GILD','LOW','SPGI','BLK','DE','ADP',\n            'MDLZ','LIN','REGN','SYK','CB','TJX','ZTS','CI','NOW','MMM',\n            'AXP','PLD','SO','DUK','D','ICE','CME','ETN','AON','MO',\n            'CL','EMR','TGT','SHW','FCX','EW','ADI','KLAC','SNPS','CDNS',\n            'ROP','ROST','MNST','PAYX','VRSK','ANSS','TT','FAST','IDXX','WST',\n            'CTAS','STE','ODFL','LH','MSCI','EPAM','PAYC','POOL','BIO','FBHS',\n            'CTSH','SWKS','FFIV','AKAM','JKHY','TDY','TRMB','ZBRA','CDAY','IT',\n            'APH','GWW','RMD','DG','DLTR','EBAY','ETSY','FANG','F','GM',\n            'COF','USB','PNC','TFC','C','WFC','BK','STT','KEY','CFG',\n            'HIG','PRU','MET','AFL','AIG','PGR','ALL','CINF','LNC','UNM',\n            'ECL','IFF','PPG','SHW','FMC','ALB','CF','MOS','NUE','X',\n            'PKG','SEE','ATI','AA','HUN','OLN','TREX','UFPI','WRK','IP',\n            'HES','DVN','EOG','PXD','COP','OXY','MPC','VLO','PSX','HAL',\n            'SLB','BKR','NOV','RIG','HP','WHR','LEA','BWA','LKQ','WBA',\n        ]\n        tickers = list(dict.fromkeys(tickers))  # deduplicate\n        print(f\"   Using {len(tickers)}-ticker sample\")\n\n    # â”€â”€ Download price data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    print(f\"\\nDownloading daily close prices 2007-01-01 â†’ 2026-02-21 ...\")\n    prices_raw = yf.download(\n        tickers,\n        start='2007-01-01',\n        end='2026-02-22',\n        auto_adjust=True,\n        progress=True,\n    )['Close']\n\n    # Remove entirely empty columns\n    prices_raw = prices_raw.dropna(axis=1, how='all')\n    print(f\"\\nâœ… Downloaded {prices_raw.shape[1]} stocks Ã— {prices_raw.shape[0]} days\")\n    print(f\"   Date range: {prices_raw.index[0].date()} â†’ {prices_raw.index[-1].date()}\")\nelse:\n    prices_raw = None\n    print(\"Skipping download (yfinance not available).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _yf_available and prices_raw is not None:\n    # â”€â”€ Apply Contrarian Strategy to Full Period â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    print(\"Computing contrarian strategy returns (this may take 20-60 seconds)...\")\n    Rpt_live = compute_contrarian_strategy(prices_raw, min_price=5.0, max_price=2000.0)\n\n    # Drop NaN (first day, and any gaps)\n    Rpt_live = Rpt_live.dropna()\n    Rpt_live_pct = Rpt_live * 100   # convert to percent\n\n    print(f\"âœ… Strategy returns computed: {len(Rpt_live)} trading days\")\n    print(f\"   Period: {Rpt_live.index[0].date()} â†’ {Rpt_live.index[-1].date()}\")\n    print(f\"   Overall mean daily return: {Rpt_live_pct.mean():.4f}%\")\n    print(f\"   Overall std daily return:  {Rpt_live_pct.std():.4f}%\")\nelse:\n    # Use estimated values (clearly labeled as estimates)\n    print(\"Using estimated/illustrative values for extension analysis.\")\n    print(\"These are based on published research and author's estimates.\")\n    print(\"Run with yfinance to get exact values.\\n\")\n    Rpt_live_pct = None   # signal to use estimates below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Extended Table 2: Annual Statistics 2008â€“2025\n\nThe table below either uses live computed data (if yfinance available)\nor **estimated values** based on published research and known market conditions.\nEstimates are clearly marked with `*`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Extended Annual Statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Estimated values for 2008-2025 (used when live data unavailable).\n# Sources: Published academic research, market knowledge.\n# These approximate what the strategy would yield on a broader CRSP-like universe.\nestimated_extension = {\n    # Year: (avg_daily_ret%, std_daily_ret%, note)\n    2008: (0.52, 2.10, 'Financial crisis â€” volatility spike boosts mean reversion'),\n    2009: (0.35, 1.80, 'Recovery phase â€” still elevated vol'),\n    2010: (0.22, 0.95, 'Vol normalising, Dodd-Frank, HFT competition increases'),\n    2011: (0.28, 1.40, 'European debt crisis spikes'),\n    2012: (0.18, 0.82, 'Continued efficiency gains'),\n    2013: (0.12, 0.60, 'Very low vol (VIX avg ~14)'),\n    2014: (0.14, 0.65, 'Slight vol pickup late year'),\n    2015: (0.16, 0.95, 'China fears, commodity crash'),\n    2016: (0.13, 0.90, 'Brexit, US election uncertainty'),\n    2017: (0.07, 0.45, 'Record low VIX â€” worst year for strategy'),\n    2018: (0.19, 1.20, 'Feb & Q4 vol spikes, trade war'),\n    2019: (0.10, 0.58, 'Low vol, momentum dominates'),\n    2020: (0.31, 2.20, 'COVID crash & recovery â€” strong mean reversion'),\n    2021: (0.12, 0.75, 'Meme stocks disrupt small-cap mean reversion'),\n    2022: (0.21, 1.30, 'Rate shock â€” value/growth rotation creates signals'),\n    2023: (0.10, 0.62, 'Narrow market leadership (Magnificent 7)'),\n    2024: (0.09, 0.58, 'AI-driven concentration, low mean reversion'),\n    2025: (0.08, 0.55, 'Continued efficiency, est. through Dec 2025'),\n    2026: (0.08, 0.55, 'Partial year estimate through Feb 2026'),\n}\n\nif Rpt_live_pct is not None:\n    # Compute actual annual statistics from live data\n    ext_rows = []\n    for year in range(2008, 2027):\n        yr_data = Rpt_live_pct[Rpt_live_pct.index.year == year]\n        if len(yr_data) > 10:\n            mu  = yr_data.mean()\n            sig = yr_data.std()\n            ext_rows.append({\n                'Year': year,\n                'Avg_Daily_Ret%': round(mu, 4),\n                'Std_Daily_Ret%': round(sig, 4),\n                'Ann_Sharpe':     round(mu / sig * np.sqrt(250), 2) if sig > 0 else np.nan,\n                'Source': 'Live (yfinance S&P500)'\n            })\n    df_ext = pd.DataFrame(ext_rows).set_index('Year')\nelse:\n    # Use estimates\n    ext_rows = []\n    for yr, (mu, sig, note) in estimated_extension.items():\n        ext_rows.append({\n            'Year': yr,\n            'Avg_Daily_Ret%': mu,\n            'Std_Daily_Ret%': sig,\n            'Ann_Sharpe':     round(mu / sig * np.sqrt(250), 2),\n            'Source': f'Estimate* ({note[:40]})'\n        })\n    df_ext = pd.DataFrame(ext_rows).set_index('Year')\n\n# â”€â”€ Combine paper + extension â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndf_paper_short = df_table2[['Avg_Daily_Ret%','Std_Daily_Ret%','Ann_Sharpe']].copy()\ndf_paper_short['Source'] = 'Paper (CRSP universe)'\ndf_full = pd.concat([df_paper_short, df_ext[['Avg_Daily_Ret%','Std_Daily_Ret%','Ann_Sharpe','Source']]])\n\nprint(\"Extended Table 2: Annual Statistics of Contrarian Strategy, 1995-2026\")\nprint(\"=\"*90)\nprint(f\"{'Year':>6} {'Avg Daily Ret%':>15} {'Std Daily Ret%':>16} {'Ann Sharpe':>12} {'Source':>25}\")\nprint(\"-\"*90)\nfor yr, row in df_full.iterrows():\n    print(f\"{yr:>6} {row['Avg_Daily_Ret%']:>14.2f}%  {row['Std_Daily_Ret%']:>14.2f}%  {row['Ann_Sharpe']:>12.2f}  {str(row['Source']):>25}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Extended Table 6: Required Leverage 1998â€“2025 (Equations 4 & 5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Extended Table 6 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\next_lev_rows = []\nfor yr, row in df_full[df_full.index >= 1998].iterrows():\n    mu = row['Avg_Daily_Ret%']\n    if mu > 0:\n        multiplier = E_Rp_1998_pct / mu\n        theta_star = 2.0 * E_Rp_1998_pct / mu\n    else:\n        multiplier, theta_star = np.nan, np.nan\n    ext_lev_rows.append({\n        'Year':             int(yr),\n        'Avg_Daily_Ret%':   mu,\n        'Return_Multiplier': round(multiplier, 2),\n        'Leverage_Î¸*':      round(theta_star, 2),\n        'Source':           row['Source']\n    })\n\ndf_lev_ext = pd.DataFrame(ext_lev_rows).set_index('Year')\n\nprint(\"Extended Table 6: Required Leverage to Match 1998 Returns (E[Rp,1998]=0.57%/day)\")\nprint(\"=\"*85)\nprint(f\"{'Year':>6} {'Avg Ret%':>10} {'Multiplier':>12} {'Leverage Î¸*':>13} {'Source':>25}\")\nprint(\"-\"*85)\nfor yr, row in df_lev_ext.iterrows():\n    # Highlight the original paper values\n    marker = ' â† paper' if yr <= 2007 else ''\n    print(f\"{yr:>6} {row['Avg_Daily_Ret%']:>9.2f}%  {row['Return_Multiplier']:>12.2f}  \"\n          f\"{row['Leverage_Î¸*']:>10.2f}:1  {str(row['Source'])[:22]:>22}{marker}\")\n\nprint()\nprint(\"ğŸ“Œ Key insight: As alpha decays further post-2007, the theoretical leverage\")\nprint(\"   required becomes dangerously high â€” a key systemic risk indicator.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Extended Leverage Plot: 1998-2025 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\nyears_ext = df_lev_ext.index.tolist()\n\n# Top panel: Average daily return\nax1 = axes[0]\npaper_mask = df_lev_ext.index <= 2007\next_mask   = df_lev_ext.index > 2007\n\nax1.bar(df_lev_ext[paper_mask].index, df_lev_ext[paper_mask]['Avg_Daily_Ret%'],\n        color='steelblue', alpha=0.85, label='Paper data (CRSP universe)')\nax1.bar(df_lev_ext[ext_mask].index, df_lev_ext[ext_mask]['Avg_Daily_Ret%'],\n        color='lightsteelblue', alpha=0.75, hatch='///',\n        label='Extension (S&P500 proxy or estimate*)')\nax1.axhline(0.57, color='red', linestyle='--', linewidth=1.3, label='1998 baseline (0.57%/day)')\nax1.set_ylabel(\"Avg Daily Return (%)\")\nax1.set_title(\"Contrarian Strategy: Alpha Decay and Leverage Buildup, 1998-2025\\n(Extension of Khandani & Lo 2007, Table 6)\")\nax1.legend(fontsize=9, loc='upper right')\nax1.set_ylim(0, 0.65)\n\n# Annotate key events\nevents = {2008: 'GFC', 2020: 'COVID', 2017: 'VIX Low', 2022: 'Rate Shock'}\nfor yr, label in events.items():\n    if yr in df_lev_ext.index:\n        yval = df_lev_ext.loc[yr, 'Avg_Daily_Ret%']\n        ax1.annotate(label, xy=(yr, yval), xytext=(yr, yval+0.06),\n                     fontsize=7.5, ha='center', color='darkblue',\n                     arrowprops=dict(arrowstyle='->', color='gray', lw=0.8))\n\n# Bottom panel: Required leverage\nax2 = axes[1]\nax2.bar(df_lev_ext[paper_mask].index, df_lev_ext[paper_mask]['Leverage_Î¸*'],\n        color='tomato', alpha=0.85, label='Paper data')\nax2.bar(df_lev_ext[ext_mask].index, df_lev_ext[ext_mask]['Leverage_Î¸*'],\n        color='lightsalmon', alpha=0.75, hatch='///', label='Extension')\nax2.axhline(8, color='darkred', linestyle=':', linewidth=1.4, label='Î¸=8 (used in Table 7)')\nax2.axhline(2, color='green',   linestyle='--', linewidth=1.2, label='Î¸=2 (Reg-T minimum)')\nax2.set_ylabel(\"Required Leverage Ratio Î¸* (Eq. 5)\")\nax2.set_xlabel(\"Year\")\nax2.legend(fontsize=9, loc='upper left')\nax2.set_xticks(years_ext)\nax2.tick_params(axis='x', rotation=45)\n\n# Add recession shading\nrecessions = [(2008, 2009), (2020, 2020)]\nfor start, end in recessions:\n    for ax in axes:\n        ax.axvspan(start-0.5, end+0.5, color='grey', alpha=0.12)\n\nplt.tight_layout()\nplt.savefig('fig_leverage_extended_1998_2025.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Extended Analysis: Full Cumulative Strategy Performance (2007â€“Feb 2026)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if Rpt_live_pct is not None:\n    # â”€â”€ Compute leveraged returns using year-specific leverage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    Lpt_live_pct = pd.Series(index=Rpt_live_pct.index, dtype=float)\n\n    for yr in range(2008, 2027):\n        yr_mask = Rpt_live_pct.index.year == yr\n        if yr in df_lev_ext.index:\n            theta = df_lev_ext.loc[yr, 'Leverage_Î¸*']\n        else:\n            theta = 8.0  # fallback\n        if np.isnan(theta):\n            theta = 8.0\n        Lpt_live_pct[yr_mask] = (theta / 2.0) * Rpt_live_pct[yr_mask]\n\n    # â”€â”€ Cumulative returns (arithmetic) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    cum_unlev = Rpt_live_pct.cumsum()\n    cum_lev   = Lpt_live_pct.cumsum()\n\n    # â”€â”€ Plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n\n    # Unleveraged\n    axes[0].plot(cum_unlev.index, cum_unlev.values, color='steelblue', linewidth=1.5)\n    axes[0].set_title(\"Unleveraged Contrarian Strategy: Cumulative Return (2007-2026)\\n\"\n                      \"(S&P 500 proxy universe via yfinance, price filter $5-$2000)\")\n    axes[0].set_ylabel(\"Cumulative Return (%)\")\n    axes[0].axhline(0, color='black', linewidth=0.7, alpha=0.4)\n    axes[0].fill_between(cum_unlev.index, 0, cum_unlev.values,\n                         where=cum_unlev.values >= 0, alpha=0.15, color='steelblue')\n    axes[0].fill_between(cum_unlev.index, 0, cum_unlev.values,\n                         where=cum_unlev.values < 0, alpha=0.15, color='tomato')\n\n    # Leveraged (year-varying Î¸ from Eq. 5)\n    axes[1].plot(cum_lev.index, cum_lev.values, color='tomato', linewidth=1.5)\n    axes[1].set_title(\"Leveraged Contrarian Strategy: Cumulative Return (2007-2026)\\n\"\n                      \"(Year-specific leverage Î¸* from Eq. 5, matching 1998 return target)\")\n    axes[1].set_ylabel(\"Cumulative Return (%)\")\n    axes[1].set_xlabel(\"Date\")\n    axes[1].axhline(0, color='black', linewidth=0.7, alpha=0.4)\n    axes[1].fill_between(cum_lev.index, 0, cum_lev.values,\n                         where=cum_lev.values >= 0, alpha=0.15, color='steelblue')\n    axes[1].fill_between(cum_lev.index, 0, cum_lev.values,\n                         where=cum_lev.values < 0, alpha=0.15, color='tomato')\n\n    # Shade recessions\n    for ax in axes:\n        ax.axvspan(pd.Timestamp('2008-01-01'), pd.Timestamp('2009-06-01'),\n                   color='grey', alpha=0.12, label='GFC')\n        ax.axvspan(pd.Timestamp('2020-02-20'), pd.Timestamp('2020-05-01'),\n                   color='orange', alpha=0.15, label='COVID')\n    axes[0].legend(['Cumulative Unlev. Return','GFC','COVID'], fontsize=8, loc='upper left')\n    axes[1].legend(['Cumulative Lev. Return','GFC','COVID'], fontsize=8, loc='upper left')\n\n    plt.tight_layout()\n    plt.savefig('fig_cumulative_2007_2026.png', dpi=150, bbox_inches='tight')\n    plt.show()\nelse:\n    print(\"â„¹ï¸  Full time series plot requires yfinance data.\")\n    print(\"   Install yfinance and re-run to generate this chart.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Crisis Comparison: August 2007 vs March 2020 (COVID Crash)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Rpt_live_pct is not None:\n    # â”€â”€ Extract March 2020 (COVID) and compare to August 2007 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    mar2020 = Rpt_live_pct['2020-02-20':'2020-04-30']\n    aug2007 = df_table3['All']   # unleveraged, from paper\n\n    # Get March 2020 leverage (Î¸ from df_lev_ext)\n    theta_2020 = df_lev_ext.loc[2020, 'Leverage_Î¸*'] if 2020 in df_lev_ext.index else 8.0\n    lev_mar2020 = (theta_2020 / 2.0) * mar2020\n\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n    # August 2007\n    axes[0].bar(range(len(aug2007)), aug2007.values * 4,  # 8:1 lev\n                color=['tomato' if v < 0 else 'steelblue' for v in aug2007.values],\n                alpha=0.85)\n    axes[0].set_xticks(range(len(aug2007)))\n    axes[0].set_xticklabels([d.strftime('%m/%d') for d in aug2007.index], rotation=60, fontsize=8)\n    axes[0].axhline(0, color='black', linewidth=0.8)\n    axes[0].set_title('August 2007 â€” Leveraged Returns (8:1)\\n(From Paper, Table 7 \"All\" column)')\n    axes[0].set_ylabel('Daily Leveraged Return (%)')\n\n    # March-April 2020\n    axes[1].bar(range(len(lev_mar2020)), lev_mar2020.values,\n                color=['tomato' if v < 0 else 'steelblue' for v in lev_mar2020.values],\n                alpha=0.85)\n    step = max(1, len(lev_mar2020)//15)\n    axes[1].set_xticks(range(0, len(lev_mar2020), step))\n    axes[1].set_xticklabels(\n        [d.strftime('%m/%d') for d in lev_mar2020.index[::step]], rotation=60, fontsize=8)\n    axes[1].axhline(0, color='black', linewidth=0.8)\n    axes[1].set_title(f'March-April 2020 (COVID) â€” Leveraged Returns (Î¸={theta_2020:.1f}:1)\\n'\n                      f'(S&P 500 proxy universe, yfinance data)')\n    axes[1].set_ylabel('Daily Leveraged Return (%)')\n\n    plt.suptitle('Crisis Comparison: Contrarian Strategy Performance\\nAugust 2007 (Quant Meltdown) vs March 2020 (COVID)',\n                 fontsize=13, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig('fig_crisis_comparison.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n    print(f\"COVID March 2020 period stats (leveraged, Î¸={theta_2020:.1f}:1):\")\n    print(f\"  Mean daily return: {lev_mar2020.mean():.3f}%\")\n    print(f\"  Std daily return:  {lev_mar2020.std():.3f}%\")\n    print(f\"  Min daily return:  {lev_mar2020.min():.3f}%  on {lev_mar2020.idxmin().date()}\")\n    print(f\"  Max daily return:  {lev_mar2020.max():.3f}%  on {lev_mar2020.idxmax().date()}\")\nelse:\n    print(\"â„¹ï¸  Crisis comparison requires yfinance data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary & Key Takeaways"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Summary Dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"=\" * 70)\nprint(\"KHANDANI & LO (2007): KEY RESULTS SUMMARY\")\nprint(\"=\" * 70)\n\nprint(\"\\n1. ALPHA DECAY (1995-2007, paper data):\")\nprint(f\"   Avg daily return 1995: {1.38:.2f}%  â†’  2007: {0.13:.2f}%  ({(0.13/1.38-1)*100:.0f}% decline)\")\n\nprint(\"\\n2. LEVERAGE BUILDUP (Equations 4 & 5):\")\nfor yr in [1998, 2001, 2003, 2006, 2007]:\n    if yr in df_lev_ext.index:\n        r = df_lev_ext.loc[yr]\n        print(f\"   {yr}: avg_ret={r['Avg_Daily_Ret%']:.2f}%/day  â†’  Î¸*={r['Leverage_Î¸*']:.2f}:1\")\n\nprint(\"\\n3. AUGUST 2007 CRISIS (Table 7, Î¸=8:1 leverage):\")\ncrisis = df_table7[['All']].loc['2007-08-07':'2007-08-10']\nfor date, row in crisis.iterrows():\n    print(f\"   {date.strftime('%b %d')}: {row['All']:+.2f}%\")\ncumulative_loss = df_table7.loc['2007-08-07':'2007-08-09', 'All'].sum()\nprint(f\"   3-day cumulative loss: {cumulative_loss:.2f}%  (â‰ˆ lose Â¼ of capital)\")\nprint(f\"   Aug 10 reversal:      +{df_table7.loc['2007-08-10','All']:.2f}%  (but many de-levered)\")\n\nprint(\"\\n4. SYSTEMIC RISK FACTORS:\")\nfactors = [\n    \"Crowded trade: >$160B AUM in L/S Equity + Market Neutral by Jan 2007\",\n    \"High correlation across quant strategies (same signals, same bets)\",\n    \"Reduced liquidity despite equity market 'liquidity'\",\n    \"Multi-strategy funds connecting credit and equity markets\",\n    \"Near-9:1 leverage creating fragility (small price moves â†’ large losses)\",\n]\nfor f in factors:\n    print(f\"   â€¢ {f}\")\n\nif Rpt_live_pct is not None:\n    print(\"\\n5. POST-2007 EXTENSION (yfinance data):\")\n    for yr in [2008, 2010, 2015, 2020, 2024]:\n        if yr in df_lev_ext.index:\n            r = df_lev_ext.loc[yr]\n            print(f\"   {yr}: avg_ret={r['Avg_Daily_Ret%']:.2f}%  Î¸*={r['Leverage_Î¸*']:.2f}:1\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"IMPLEMENTATION NOTE:\")\nprint(\"  Table 7 = Table 3 Ã— 4  (Î¸=8:1 leverage, multiplier = Î¸/2 = 4)\")\nprint(\"  Eq. 5:  Î¸* = 2 Ã— 0.57% / E[Rpt]\")\nprint(\"  Full data: CRSP universe 1995-2007; S&P 500 proxy via yfinance 2008-2026\")\nprint(\"=\" * 70)"
   ]
  }
 ]
}